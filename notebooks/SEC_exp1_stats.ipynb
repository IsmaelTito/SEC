{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import ast\n",
    "import statistics\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import random as nd\n",
    "import pickle as pkl\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 600\n",
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main data parsing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot_data(obj, name, path):\n",
    "    with open(path+name+'.pkl', 'wb') as f:\n",
    "        pkl.dump(obj, f, pkl.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_plot_data(name, path):\n",
    "    with open(path+name+'.pkl', 'rb') as f:\n",
    "        return pkl.load(f)\n",
    "\n",
    "def load_json_data(inputfile):\n",
    "    summ_data = []\n",
    "    \n",
    "    with open(inputfile) as json_file:\n",
    "        summ_data = json.load(json_file)\n",
    "        print('agent_ID: ' + summ_data['agent_ID'])\n",
    "        #print('agent_type: ' + summ_data['agent_type'])\n",
    "        #print('total_episodes: ' + str(summ_data['total_episodes']))\n",
    "        #print('total_reward: ' + str(summ_data['total_reward']))\n",
    "        #print('avg_rec_err: ' + str(summ_data['avg_rec_err']))\n",
    "        #print('CL_ratio: ' + str(summ_data['CL_ratio']))\n",
    "        #print('STM_length: ' + str(summ_data['STM_length']))\n",
    "        #print('rec_thres: ' + str(summ_data['rec_thres']))\n",
    "        #print('mean_step_rew: ' + str(summ_data['mean_step_rew']))\n",
    "        #print('mean_step_log: ' + data['mean_step_log'])\n",
    "        #print('wins_reactive: ' + str(summ_data['wins_reactive']))\n",
    "        #print('wins_contextual: ' + str(summ_data['wins_contextual']))\n",
    "        #print('auto_reliable: ' + str(summ_data['auto_reliable']))\n",
    "\n",
    "    return summ_data\n",
    "\n",
    "def load_csv_data(inputfile):\n",
    "\n",
    "    # initializing the titles and rows list \n",
    "    fields = [] \n",
    "    data = [] \n",
    "\n",
    "    # reading csv file \n",
    "    with open(inputfile, 'r') as csvfile: \n",
    "        # creating a csv reader object \n",
    "        csvreader = csv.reader(csvfile) \n",
    "\n",
    "        # extracting field names through first row  \n",
    "        fields = next(csvreader) \n",
    "        \n",
    "        # printing the field names \n",
    "        #print('Field names are: ' + ', '.join(field for field in fields)) \n",
    "        \n",
    "        # extracting each data row one by one \n",
    "        for row in csvreader: \n",
    "            if len(row) > 0:\n",
    "                data.append(row) \n",
    "               \n",
    "        #print('Data length: ', len(data))\n",
    "\n",
    "        # get total number of rows \n",
    "        print(\"Total no. of rows: %d\"%(csvreader.line_num)) \n",
    "    \n",
    "    print(\"Data length: \", len(data))  \n",
    "\n",
    "    return data\n",
    "\n",
    "def get_reward_ep(data):\n",
    "    reward_ep = []\n",
    "    for d in data:\n",
    "        reward_ep.append(d[0])\n",
    "    return reward_ep\n",
    "\n",
    "def get_rec_error_mean(data):\n",
    "    rec_error_mean = []\n",
    "    for d in data:\n",
    "        rec_error_mean.append(d[1])\n",
    "    return rec_error_mean\n",
    "\n",
    "def get_reward_logs(data):\n",
    "    reward_logs = []\n",
    "    for d in data:\n",
    "        reward_logs.append(d[2])\n",
    "    return reward_logs\n",
    "\n",
    "def get_rec_error_logs(data):\n",
    "    rec_error_logs = []\n",
    "    for d in data:\n",
    "        rec_error_logs.append(d[3])\n",
    "    return rec_error_logs\n",
    "\n",
    "def get_agent_speed_logs(data):\n",
    "    agent_speed_logs = []\n",
    "    for d in data:\n",
    "        agent_speed_logs.append(d[4])\n",
    "    return agent_speed_logs\n",
    "\n",
    "def get_agent_pos_logs(data):\n",
    "    agent_pos_logs = []\n",
    "    for d in data:\n",
    "        agent_pos_logs.append(d[5])\n",
    "    return agent_pos_logs\n",
    "\n",
    "def get_active_layer_logs(data, model):\n",
    "    active_layer_logs = []\n",
    "    for d in data:\n",
    "        active_layer_logs.append(d[6])\n",
    "        #active_layer_logs.append(d[6]) if model != 'MFEC' else active_layer_logs.append(d[5])\n",
    "    return active_layer_logs\n",
    "\n",
    "def get_entropy_logs(data, model):\n",
    "    entropy_logs = []\n",
    "    for d in data:\n",
    "        entropy_logs.append(d[6])\n",
    "        #if model != 'MFEC': entropy_logs.append(d[7]) \n",
    "    return entropy_logs\n",
    "\n",
    "def get_action_sel_logs(data, model):\n",
    "    action_sel_logs = []\n",
    "    for d in data:\n",
    "        action_sel_logs.append(d[8])\n",
    "        #action_sel_logs.append(d[8]) if model != 'MFEC' else action_sel_logs.append(d[6])\n",
    "    return action_sel_logs\n",
    "\n",
    "def get_reward_ts(reward_ep, ltm_length):\n",
    "    #ltm = int(ltm_length[3:])\n",
    "    ltm = ltm_length\n",
    "\n",
    "    abs_tru_reward_ts = []\n",
    "    abs_rnd_reward_ts = []\n",
    "    acc_tru_reward = 0\n",
    "    acc_tru_reward_ts = []\n",
    "    acc_rnd_reward = 0\n",
    "    acc_rnd_reward_ts = []\n",
    "    count_ltm = 0\n",
    "    index_eps = 0\n",
    "    ltm_complete = 0\n",
    "    \n",
    "    for episode in reward_ep:\n",
    "        #print(episode)\n",
    "        r = float(episode)\n",
    "        \n",
    "        #print(r)\n",
    "        if r > 0:\n",
    "            #print(r)\n",
    "            abs_tru_reward_ts.append(r)\n",
    "            abs_rnd_reward_ts.append(3)\n",
    "            acc_tru_reward += r\n",
    "            acc_rnd_reward += 3\n",
    "            count_ltm += 1\n",
    "        else:\n",
    "            abs_tru_reward_ts.append(0)\n",
    "            abs_rnd_reward_ts.append(0)\n",
    "            acc_tru_reward += 0\n",
    "            acc_rnd_reward += 0\n",
    "            \n",
    "        if count_ltm == ltm: \n",
    "            ltm_complete = index_eps\n",
    "            ltm = -1\n",
    "            \n",
    "        acc_tru_reward_ts.append(acc_tru_reward)\n",
    "        acc_rnd_reward_ts.append(acc_rnd_reward)\n",
    "        index_eps += 1\n",
    "        \n",
    "    return abs_tru_reward_ts, abs_rnd_reward_ts, acc_tru_reward_ts, acc_rnd_reward_ts, ltm_complete\n",
    "\n",
    "\n",
    "def get_steps_ts(reward_ep, steps_data, cl=4):\n",
    "    #default episode total actions: \n",
    "    # CL4 - 251, CL2 - 501, # CL1 - 1001\n",
    "    #print(cl[1])\n",
    "    #default_steps = round(1000 / int(cl[1:]))\n",
    "    default_steps = round(1000 / cl)\n",
    "    steps_index = 0\n",
    "    ep_steps_ts = []\n",
    "    acc_steps = 0\n",
    "    acc_steps_ts = []\n",
    "    steps = steps_data\n",
    "    #steps = json_data['mean_step_log']\n",
    "    #for episode in reward_ep:\n",
    "    for episode in reward_ep:\n",
    "        #print(episode)\n",
    "        r = float(episode)\n",
    "        #print(r)\n",
    "        if r > 0:\n",
    "            ep_steps_ts.append(int(steps[steps_index]))\n",
    "            acc_steps += int(steps[steps_index])\n",
    "            steps_index += 1 \n",
    "        else:\n",
    "            ep_steps_ts.append(default_steps)\n",
    "            acc_steps += default_steps\n",
    "            \n",
    "        acc_steps_ts.append(acc_steps)    \n",
    "                    \n",
    "    return ep_steps_ts, acc_steps_ts\n",
    "\n",
    "def get_condition_data(cl, stm):\n",
    "    data = []\n",
    "    for r,d,f in os.walk(base_path):\n",
    "        for files in f:\n",
    "            if files.startswith('c'+str(cl)+'-stm'+str(stm)) and files.endswith('.csv'):\n",
    "                #ts = []\n",
    "                inputfile = r + files\n",
    "                print(\"r: \", r)\n",
    "                data = load_data(inputfile)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_layer_reliance_ep(layer_logs):\n",
    "    reactive_reliance = []\n",
    "    contextual_reliance = []\n",
    "    episode_dominance = []\n",
    "    episode_last = []\n",
    "    for ep in layer_logs:\n",
    "        contextual = 0\n",
    "        reactive = 0\n",
    "        for d in ep:\n",
    "            if d == 'C': contextual += 1\n",
    "            if d == 'R': reactive += 1\n",
    "        if contextual >= reactive: episode_dominance.append('C')\n",
    "        else: episode_dominance.append('R')\n",
    "        if (reactive+contextual) > 0:\n",
    "            rel_reactive = reactive / (reactive+contextual)\n",
    "            rel_contextual = contextual / (reactive+contextual)\n",
    "        else:\n",
    "            rel_reactive = 0\n",
    "            rel_contextual = 0\n",
    "        reactive_reliance.append(rel_reactive)\n",
    "        contextual_reliance.append(rel_contextual)\n",
    "        episode_last.append(ep[-3])\n",
    "    return episode_dominance, episode_last, reactive_reliance, contextual_reliance\n",
    "\n",
    "\n",
    "def get_layer_reward_ts_last(reward_ep, steps_data, ep_last):\n",
    "    count_reactive = 0\n",
    "    abs_reward_reactive = []\n",
    "    abs_reward_contextual = []\n",
    "    acc_contextual_reward = 0\n",
    "    acc_reactive_reward = 0\n",
    "    acc_reward_reactive = []\n",
    "    acc_reward_contextual = []\n",
    "    #count_steps = 0\n",
    "    steps_reactive = []\n",
    "    steps_contextual = []\n",
    "    steps = steps_data\n",
    "    #steps = json_data['mean_step_log']\n",
    "    #for episode in reward_ep:\n",
    "    for i in range (len(reward_ep)):\n",
    "        #print(episode)\n",
    "        r = float(reward_ep[i])\n",
    "        #print(r)\n",
    "        if r >= 1.:\n",
    "            #if count_reactive > 4:\n",
    "            if ep_last[i] == 'R':\n",
    "                acc_reactive_reward += r\n",
    "                abs_reward_reactive.append(r)\n",
    "                acc_reward_reactive.append(acc_reactive_reward)\n",
    "                steps_reactive.append(steps[count_reactive])\n",
    "                \n",
    "                abs_reward_contextual.append(0)\n",
    "                acc_reward_contextual.append(acc_contextual_reward)\n",
    "            else:\n",
    "                acc_contextual_reward += r\n",
    "                abs_reward_contextual.append(r)\n",
    "                acc_reward_contextual.append(acc_contextual_reward)\n",
    "                steps_contextual.append(steps[count_reactive])  \n",
    "                \n",
    "                abs_reward_reactive.append(0)\n",
    "                acc_reward_reactive.append(acc_reactive_reward)\n",
    "                    \n",
    "            count_reactive += 1                \n",
    "            #count_steps += 1  \n",
    "        \n",
    "    return abs_reward_reactive, acc_reward_reactive, abs_reward_contextual, acc_reward_contextual, steps_reactive, steps_contextual\n",
    "\n",
    "def get_mean_entropy(data):\n",
    "    mean_entropy_ep = []\n",
    "    for d in data:\n",
    "        #a = np.fromstring(d[1:-1], dtype=float, sep=',')\n",
    "        float_array = ast.literal_eval(d)\n",
    "        mean_entropy = np.mean(float_array)\n",
    "        mean_entropy_ep.append(mean_entropy)\n",
    "    return mean_entropy_ep\n",
    "\n",
    "def get_encoder_reliability_ts(rec_error_logs, rec_thresh=0):\n",
    "    data = rec_error_logs\n",
    "    reconstruct_thres = rec_thresh\n",
    "    encoder_reliability_ts = []\n",
    "\n",
    "    for ep in data:\n",
    "        a = np.fromstring(ep[1:-1], dtype=float, sep=',')\n",
    "        #print(len(a))\n",
    "        count_reliable = 0\n",
    "        count_unreliable = 0\n",
    "        encoder_reliability = 0\n",
    "        for rec_error_ts in a:\n",
    "            if rec_error_ts < reconstruct_thres: \n",
    "                #print ('Reconstruction Error RELIABLE')\n",
    "                count_reliable +=1  \n",
    "            else:\n",
    "                #print ('Reconstruction Error UNRELIABLE') \n",
    "                count_unreliable +=1\n",
    "        encoder_reliability = (count_reliable / (count_reliable+count_unreliable)) * 100\n",
    "        encoder_reliability_ts.append(encoder_reliability)\n",
    "    return encoder_reliability_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_condition_dac(filepath, model, ltm_length=500):\n",
    "    \n",
    "    agent = \"dac\"\n",
    "    indx = 0\n",
    "    jsn_data = 0\n",
    "    agent_ID = \"\"\n",
    "    \n",
    "    rec_threshold=0.01\n",
    "\n",
    "    #big_list = []\n",
    "    big_json = []\n",
    "    big_steps = []\n",
    "    big_autoencoder_trust_rel, big_autoencoder_trust_ts = [], []\n",
    "    big_reward_ep = []\n",
    "    big_abs_tru_reward, big_abs_rnd_reward, big_acc_tru_reward, big_acc_rnd_reward = [], [], [], []\n",
    "    big_layer_data = []\n",
    "    big_ep_dominance, big_ep_last, big_reactive_rel, big_contextual_rel =  [], [], [], []\n",
    "    big_abs_rew_reactive, big_acc_rew_reactive, big_abs_rew_contextual, big_acc_rew_contextual, big_steps_reactive, big_steps_contextual = [], [], [], [], [], []\n",
    "    big_rec_error_logs = []\n",
    "    big_wins_reactive, big_wins_contextual = [], []\n",
    "    #big_autoencoder_trust_ts = []  \n",
    "    big_entropy_data, big_mean_entropy = [], []\n",
    "    big_real_steps, big_acc_steps = [], []\n",
    "    big_ltm_complete = []\n",
    "    big_pos = []\n",
    "\n",
    "    print(filepath+'{0}/'.format(model))\n",
    "    for r,d,f in os.walk(filepath+'{0}/'.format(model)):\n",
    "        #print(\"oink! 0\")\n",
    "        for files in f:\n",
    "            if files.endswith('.json'):\n",
    "                filename = r+'/'+files\n",
    "                jsn_data = load_json_data(filename)\n",
    "                agent_ID = jsn_data['agent_ID']\n",
    "                agent_type = jsn_data['agent_type']\n",
    "                big_json.append(jsn_data)\n",
    "                big_autoencoder_trust_rel.append(jsn_data['auto_reliable'])\n",
    "                mean_steps_log = jsn_data['mean_step_log']\n",
    "                big_steps.append(mean_steps_log)\n",
    "                big_wins_reactive.append(jsn_data['wins_reactive'])\n",
    "                big_wins_contextual.append(jsn_data['wins_contextual'])\n",
    "                #print(\"oink! 1: \", agent_ID)\n",
    "                for r,d,f in os.walk(filepath+'{0}/'.format(model)):\n",
    "                    #print(\"oink! 2a\")\n",
    "                    for files in f:\n",
    "                        if files.startswith(agent_ID) and files.endswith('.csv'):\n",
    "                            #print(\"oink! 2b: \", agent_ID)\n",
    "                            filename = r+'/'+files\n",
    "                            #print(\"oink! 2c: \", filename)\n",
    "                            csv_data = load_csv_data(filename)\n",
    "                            reward_ep = get_reward_ep(csv_data)\n",
    "                            big_reward_ep.append(reward_ep)\n",
    "                            abs_tru_reward, abs_rnd_reward, acc_tru_reward, acc_rnd_reward, ltm_full = get_reward_ts(reward_ep, ltm_length)\n",
    "                            big_abs_tru_reward.append(abs_tru_reward)\n",
    "                            big_abs_rnd_reward.append(abs_rnd_reward)\n",
    "                            big_acc_tru_reward.append(acc_tru_reward)\n",
    "                            big_acc_rnd_reward.append(acc_rnd_reward)\n",
    "                            big_ltm_complete.append(ltm_full)\n",
    "                            if agent_type != \"reactive\": \n",
    "                                rec_error_logs = get_rec_error_logs(csv_data)\n",
    "                                big_rec_error_logs.append(rec_error_logs)\n",
    "                                #autoencoder_trust_ts = get_encoder_reliability_ts(rec_error_logs, rec_threshold)\n",
    "                                #big_autoencoder_trust_ts.apppend(autoencoder_trust_ts)\n",
    "                                entropy_data = get_entropy_logs(csv_data, agent_type)\n",
    "                                mean_entropy = get_mean_entropy(entropy_data)\n",
    "                                big_entropy_data.append(entropy_data)\n",
    "                                big_mean_entropy.append(mean_entropy)\n",
    "                            #if model == \"dac-pos\" or model == \"dac-nobias-pos\":\n",
    "                                #agent_pos_logs = get_agent_pos_logs(csv_data)\n",
    "                                #big_pos.append(agent_pos_logs)\n",
    "                            indx += 1\n",
    "                      \n",
    "    big_list = {}\n",
    "\n",
    "    #big_list['agent_ID'] = ID\n",
    "    big_list['abs_tru_reward'] = big_abs_tru_reward\n",
    "    big_list['acc_tru_reward'] = big_acc_tru_reward  #this\n",
    "    big_list['reward_steps_ts'] = big_steps\n",
    "    big_list['real_steps_ts'] = big_real_steps  #this\n",
    "    big_list['acc_steps_ts'] = big_acc_steps  #this\n",
    "    big_list['mean_entropy'] = big_mean_entropy #this\n",
    "    big_list['LTM_full'] = big_ltm_complete \n",
    "                \n",
    "    return big_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/exp1_2tmaze/'\n",
    "plot_path = '../data/exp1_2tmaze/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = load_plot_data('data', file_path)\n",
    "data = load_plot_data('data', file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['MFEC-ae', 'MFEC-rp', 'SEC', 'NSEC', 'DQN', 'ERLAM'])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['abs_tru_reward', 'acc_tru_reward', 'reward_steps_ts', 'real_steps_ts', 'acc_steps_ts', 'mean_entropy', 'LTM_full', 'reactive_relact', 'contextual_relact', 'reactive_wins', 'contextual_wins', 'abs_rew_reactive', 'abs_rew_contextual', 'acc_rew_reactive', 'acc_rew_contextual', 'steps_reactive', 'steps_contextual'])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['SEC'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewards - all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models = ['SEC_500', 'SEC_1000', 'MFEC_VAE_25K', 'MFEC_RP_25K', 'MFEC_VAE_100K', 'MFEC_RP_100K', 'DQN_5k', 'DQN_25k', 'ERLAM_5k', 'ERLAM_25k']\n",
    "#models = ['noGi_500', 'NSEC_500', 'NSEC_1000', 'SEC_500', 'SEC_1000', 'MFEC_VAE_25K', 'MFEC_RP_25K', 'MFEC_VAE_100K', 'MFEC_RP_100K', 'DQN_5k', 'DQN_25k', 'ERLAM_5k', 'ERLAM_25k']\n",
    "#models = ['NSEC_500', 'SEC_500', 'SEC_1000', 'MFEC_VAE_25K', 'MFEC_RP_25K', 'MFEC_VAE_100K', 'MFEC_RP_100K', 'DQN_5k', 'DQN_25k', 'ERLAM_5k', 'ERLAM_25k']\n",
    "#models = ['noGi_500', 'NSEC_500', 'SEC_500', 'SEC_1000', 'MFEC_VAE_25K', 'MFEC_RP_25K', 'MFEC_VAE_100K', 'MFEC_RP_100K', 'ERLAM_5k', 'ERLAM_25k', 'DQN_5k', 'DQN_25k']\n",
    "#models = ['DQN_ae100_cylinder', 'DQN_sparse100_cylinder', 'DQN_ae1k_cylinder', 'DQN_sparse1k_cylinder', 'MFEC_ae100_cylinder', 'MFEC_sparse100_cylinder', 'SEC_ae100_cylinder', 'SEC_sparse100_cylinder']\n",
    "#models = ['DQN_ae100_cylinder', 'DQN_sparse100_cylinder', 'MFEC_ae100_cylinder', 'MFEC_sparse100_cylinder', 'SEC_ae100_cylinder', 'SEC_sparse100_cylinder']\n",
    "#models = ['noGi_500', 'NSEC_500', 'NSEC_1000', 'SEC_500', 'SEC_1000', 'MFEC_VAE_25K', 'MFEC_RP_25K', 'MFEC_VAE_100K', 'MFEC_RP_100K', 'ERLAM_5k', 'ERLAM_25k', 'DQN_5k', 'DQN_25k']\n",
    "#models = ['DQN_ae100_'+task, 'DQN_sparse100_'+task, 'MFEC_ae100_'+task, 'MFEC_sparse100_'+task, 'SEC_ae100_'+task, 'SEC_sparse100_'+task]\n",
    "#models = ['DQN_ae1k_'+task, 'DQN_sparse1k_'+task]\n",
    "data = load_plot_data('data', file_path)\n",
    "models = ['MFEC-ae', 'MFEC-rp', 'SEC', 'NSEC', 'DQN', 'ERLAM']\n",
    "ltm = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import mannwhitneyu\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_truerward_sum  232097.61\n",
      "data_truerward shape  (20, 5000)\n",
      "last mean 1 2.78515\n",
      "last mean 2 2.78515\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "data_trureward = data['SEC']['abs_tru_reward']\n",
    "data_trureward = np.asarray(data_trureward, dtype=np.float32)\n",
    "print(\"data_truerward_sum \", np.sum(data_trureward))\n",
    "print(\"data_truerward shape \", np.shape(data_trureward))\n",
    "\n",
    "if np.sum(data_trureward) > 0:\n",
    "    if len(data_trureward[0]) > 5000: \n",
    "        data_half = np.hsplit(data_trureward, 2)\n",
    "        data_trureward = data_half[0]\n",
    "\n",
    "    mean = np.mean(data_trureward, axis=0)\n",
    "    print(\"last mean 1\", mean[-1])\n",
    "    last_elements = data_trureward[:, -1]\n",
    "    last_mean = np.mean(last_elements)\n",
    "    print(\"last mean 2\", last_mean)\n",
    "    print(len(last_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_truerward_sum  70509.586\n",
      "reward mean last  1.0884\n",
      "reward std last  1.33557\n",
      "reward yerr last 0.29864252\n",
      "reward yerr last 2  0.2986425258979946\n",
      "total reward length  20\n",
      "total reward  [2391.732    593.8881    10.793   9521.001   8964.163   5782.039\n",
      " 4974.2354  2598.361    431.44202  208.48201   11.875   4494.66\n",
      "  540.274   5921.997   3785.2139  5425.5176  4627.433   1876.476\n",
      " 2597.143   5752.8604 ]\n",
      "data_truerward_sum  22324.096\n",
      "reward mean last  0.717\n",
      "reward std last  1.2418815\n",
      "reward yerr last 0.35850027\n",
      "reward yerr last 2  0.3585003062054628\n",
      "total reward length  12\n",
      "total reward  [8222.271    7534.708      10.856001   11.925001   11.360001   11.831001\n",
      "  821.27704    12.305      44.953003  406.077      11.666001 5224.865   ]\n",
      "data_truerward_sum  232097.61\n",
      "reward mean last  2.78515\n",
      "reward std last  0.124701716\n",
      "reward yerr last 0.027884152\n",
      "reward yerr last 2  0.027884151389984718\n",
      "total reward length  20\n",
      "total reward  [12761.725  11985.011  11172.252  12745.375  12343.921  10913.979\n",
      " 11974.027  11929.746  12451.729   6251.8413 12884.659  11486.684\n",
      " 11635.516  12759.763  12604.989  12912.76   10759.244  11210.25\n",
      " 13151.1875  8162.9517]\n",
      "data_truerward_sum  100490.64\n",
      "reward mean last  1.25415\n",
      "reward std last  1.2728102\n",
      "reward yerr last 0.28460905\n",
      "reward yerr last 2  0.2846090175982817\n",
      "total reward length  20\n",
      "total reward  [6278.288  2361.041  8550.084  8521.58   8303.496  5712.9253 3163.7612\n",
      " 9974.204  1891.5762 3902.8364 5663.254  1948.2001 2713.2512  615.5101\n",
      " 4319.627  1866.707  5855.152  7796.7686 9360.873  1691.493 ]\n",
      "data_truerward_sum  62076.54\n",
      "reward mean last  1.9991\n",
      "reward std last  1.3089341\n",
      "reward yerr last 0.4139213\n",
      "reward yerr last 2  0.4139213039414039\n",
      "total reward length  10\n",
      "total reward  [10397.407    5608.31    10915.633    7101.057   11400.885     340.437\n",
      "   627.95496  9772.275     790.94806  5121.634  ]\n",
      "data_truerward_sum  78262.09\n",
      "reward mean last  1.5838001\n",
      "reward std last  1.3031106\n",
      "reward yerr last 0.41207975\n",
      "reward yerr last 2  0.4120797537583833\n",
      "total reward length  10\n",
      "total reward  [6686.9805   445.54004 7112.5073  9204.303   8387.244   9718.651\n",
      " 9766.963   8855.465   9511.042   8573.397  ]\n"
     ]
    }
   ],
   "source": [
    "task = 'DTmaze'\n",
    "models = ['MFEC-ae', 'MFEC-rp', 'SEC', 'NSEC', 'DQN', 'ERLAM']\n",
    "\n",
    "reward_data = {}\n",
    "window_width = 20\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    data_trureward = data[model]['abs_tru_reward']\n",
    "    data_trureward = np.asarray(data_trureward, dtype=np.float32)\n",
    "    print(\"data_truerward_sum \", np.sum(data_trureward))\n",
    "\n",
    "    if np.sum(data_trureward) > 0:\n",
    "        if len(data_trureward[0]) > 5000: \n",
    "            data_half = np.hsplit(data_trureward, 2)\n",
    "            data_trureward = data_half[0]\n",
    "\n",
    "        mean = np.mean(data_trureward, axis=0)\n",
    "        std = np.std(data_trureward, axis=0)\n",
    "        \n",
    "        last_elements = data_trureward[:, -1]\n",
    "        #last_mean = np.mean[-1]\n",
    "        last_mean = np.mean(last_elements)\n",
    "        #last_std = std[-1]\n",
    "        last_std = np.std(last_elements)\n",
    "        #print(\"reward mean length \", len(mean))\n",
    "        print(\"reward mean last \", last_mean)\n",
    "        print(\"reward std last \", last_std)\n",
    "\n",
    "        yerr = np.std(data_trureward, axis=0) / math.sqrt(len(data_trureward))\n",
    "        last_yerr = np.std(last_elements) / math.sqrt(len(last_elements))\n",
    "        print(\"reward yerr last\", yerr[-1])\n",
    "        print(\"reward yerr last 2 \", last_yerr)\n",
    "\n",
    "        data_trureward_cumsum = np.cumsum(np.insert(mean, 0, 0)) \n",
    "        data_trureward_mavecr = (data_trureward_cumsum[window_width:] - data_trureward_cumsum[:-window_width]) / window_width\n",
    "\n",
    "        data_trureward_cumsum_std = np.cumsum(np.insert(std, 0, 0)) \n",
    "        data_trureward_mavecr_std = (data_trureward_cumsum_std[window_width:] - data_trureward_cumsum_std[:-window_width]) / window_width\n",
    "\n",
    "        data_trureward_cumsum_yerr = np.cumsum(np.insert(yerr, 0, 0)) \n",
    "        data_trureward_mavecr_yerr = (data_trureward_cumsum_yerr[window_width:] - data_trureward_cumsum_yerr[:-window_width]) / window_width\n",
    "\n",
    "        data_totalreward = np.sum(data_trureward, axis=1) \n",
    "        print(\"total reward length \", len(data_totalreward))\n",
    "        print(\"total reward \", data_totalreward)\n",
    "        data_totalreward_mean = np.mean(data_totalreward)\n",
    "        data_totalreward_std = np.std(data_totalreward)\n",
    "        data_totalreward_yerr = data_totalreward_std / math.sqrt(len(data_totalreward))\n",
    "\n",
    "        reward = {}\n",
    "        reward['total'] = data_totalreward\n",
    "        reward['total_mean'] = data_totalreward_mean\n",
    "        reward['total_std'] = data_totalreward_std\n",
    "        reward['total_yerr'] = data_totalreward_yerr\n",
    "        reward['rwds_last'] = last_elements\n",
    "        reward['mean_hist'] = mean\n",
    "        reward['mean_last'] = last_mean\n",
    "        reward['std_hist'] = std\n",
    "        reward['std_last'] = last_std\n",
    "        reward['yerr_hist'] = yerr\n",
    "        reward['yerr_last'] = last_yerr\n",
    "        reward['mean'] = np.mean(mean)\n",
    "        reward['mavecr'] = data_trureward_mavecr\n",
    "        reward['mavecr_std'] = data_trureward_cumsum_std\n",
    "        reward['mavecr_yerr'] = data_trureward_mavecr_yerr\n",
    "        reward['min_val'] = data_trureward_mavecr - data_trureward_mavecr_yerr\n",
    "        reward['max_val'] = data_trureward_mavecr + data_trureward_mavecr_yerr\n",
    "        reward['length'] = data_trureward_mavecr_std.shape[0]\n",
    "\n",
    "        reward_data[model] = reward\n",
    "\n",
    "save_plot_data(reward_data, 'reward_new_'+task, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['MFEC', 'SEC', 'SEC-FIFO', 'SEC-RWD', 'NSEC', 'NSEC-FIFO', 'NSEC-RWD', 'SEC-FIFO_v2', 'SEC-RWD_v2', 'SEC-RWD_v3', 'SEC-RWD_v4', 'SEC-PRIOR'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = load_plot_data('data', file_path)\n",
    "file_path = '../data/exp4_forgetting/'\n",
    "plot_path = '../data/exp4_forgetting/'\n",
    "data = load_plot_data('data_new', file_path)\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL:  SEC\n",
      "data_truerward_sum  232097.61\n",
      "reward mean last  2.78515\n",
      "reward std last  0.124701716\n",
      "reward yerr last 0.027884152\n",
      "reward yerr last 2  0.027884151389984718\n",
      "total reward length  20\n",
      "total reward  [12761.725  11985.011  11172.252  12745.375  12343.921  10913.979\n",
      " 11974.027  11929.746  12451.729   6251.8413 12884.659  11486.684\n",
      " 11635.516  12759.763  12604.989  12912.76   10759.244  11210.25\n",
      " 13151.1875  8162.9517]\n",
      "MODEL:  SEC-PRIOR\n",
      "data_truerward_sum  243838.2\n",
      "reward mean last  2.6044497\n",
      "reward std last  0.6151484\n",
      "reward yerr last 0.13755135\n",
      "reward yerr last 2  0.1375513694780533\n",
      "total reward length  20\n",
      "total reward  [13000.182  12393.184  13217.7705 11684.592  13195.868  12461.193\n",
      " 12658.344  13015.821  12118.3    12056.24   11674.811  11980.355\n",
      " 11859.637  10715.049  12475.845  12769.166  12519.915  12344.092\n",
      " 12472.881   9224.965 ]\n",
      "MODEL:  SEC-FIFO_v2\n",
      "data_truerward_sum  248028.42\n",
      "reward mean last  2.7922003\n",
      "reward std last  0.14212337\n",
      "reward yerr last 0.03177975\n",
      "reward yerr last 2  0.03177975195584132\n",
      "total reward length  20\n",
      "total reward  [ 8957.194 12524.709 13302.619 12970.974 12730.531 12956.311 12544.273\n",
      " 12207.541 12853.281 12672.7   11173.451 12338.872 13026.871 10614.602\n",
      " 13383.947 12594.234 12092.469 12772.312 13122.855 13188.683]\n",
      "MODEL:  NSEC\n",
      "data_truerward_sum  100490.64\n",
      "reward mean last  1.25415\n",
      "reward std last  1.2728102\n",
      "reward yerr last 0.28460905\n",
      "reward yerr last 2  0.2846090175982817\n",
      "total reward length  20\n",
      "total reward  [6278.288  2361.041  8550.084  8521.58   8303.496  5712.9253 3163.7612\n",
      " 9974.204  1891.5762 3902.8364 5663.254  1948.2001 2713.2512  615.5101\n",
      " 4319.627  1866.707  5855.152  7796.7686 9360.873  1691.493 ]\n",
      "MODEL:  NSEC-FIFO\n",
      "data_truerward_sum  103469.84\n",
      "reward mean last  1.6178501\n",
      "reward std last  1.328572\n",
      "reward yerr last 0.29707775\n",
      "reward yerr last 2  0.29707773828981016\n",
      "total reward length  20\n",
      "total reward  [ 5280.8486   9263.691   10245.053    9036.288     494.90405  3988.0732\n",
      "  9372.098    6136.883   11992.682    1811.1951   4233.091    3837.0261\n",
      "  3502.568    2816.469    2540.9893   1583.885     200.949    4307.6973\n",
      "  6910.758    5914.7    ]\n",
      "MODEL:  NSEC-RWD\n",
      "data_truerward_sum  133381.92\n",
      "reward mean last  2.0196\n",
      "reward std last  1.180344\n",
      "reward yerr last 0.26393297\n",
      "reward yerr last 2  0.26393293885397257\n",
      "total reward length  20\n",
      "total reward  [ 3934.445    613.844   6189.8926  5029.335   5744.7754  3050.5242\n",
      " 10732.958   5842.418   5721.3604  7565.091   8165.3975  9740.015\n",
      "  7142.3535 11206.048   3610.6104  3576.8513  6784.417   9143.499\n",
      "  9317.104  10270.98  ]\n"
     ]
    }
   ],
   "source": [
    "task = 'Forgetting'\n",
    "models = ['SEC', 'SEC-PRIOR', 'SEC-FIFO_v2', 'NSEC', 'NSEC-FIFO', 'NSEC-RWD']\n",
    "\n",
    "reward_data = {}\n",
    "window_width = 20\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    data_trureward = data[model]['abs_tru_reward']\n",
    "    data_trureward = np.asarray(data_trureward, dtype=np.float32)\n",
    "    print(\"MODEL: \", model)\n",
    "    print(\"data_truerward_sum \", np.sum(data_trureward))\n",
    "\n",
    "    if np.sum(data_trureward) > 0:\n",
    "        if len(data_trureward[0]) > 5000: \n",
    "            data_half = np.hsplit(data_trureward, 2)\n",
    "            data_trureward = data_half[0]\n",
    "\n",
    "        mean = np.mean(data_trureward, axis=0)\n",
    "        std = np.std(data_trureward, axis=0)\n",
    "        \n",
    "        last_elements = data_trureward[:, -1]\n",
    "        #last_mean = np.mean[-1]\n",
    "        last_mean = np.mean(last_elements)\n",
    "        #last_std = std[-1]\n",
    "        last_std = np.std(last_elements)\n",
    "        #print(\"reward mean length \", len(mean))\n",
    "        print(\"reward mean last \", last_mean)\n",
    "        print(\"reward std last \", last_std)\n",
    "\n",
    "        yerr = np.std(data_trureward, axis=0) / math.sqrt(len(data_trureward))\n",
    "        last_yerr = np.std(last_elements) / math.sqrt(len(last_elements))\n",
    "        print(\"reward yerr last\", yerr[-1])\n",
    "        print(\"reward yerr last 2 \", last_yerr)\n",
    "\n",
    "        data_trureward_cumsum = np.cumsum(np.insert(mean, 0, 0)) \n",
    "        data_trureward_mavecr = (data_trureward_cumsum[window_width:] - data_trureward_cumsum[:-window_width]) / window_width\n",
    "\n",
    "        data_trureward_cumsum_std = np.cumsum(np.insert(std, 0, 0)) \n",
    "        data_trureward_mavecr_std = (data_trureward_cumsum_std[window_width:] - data_trureward_cumsum_std[:-window_width]) / window_width\n",
    "\n",
    "        data_trureward_cumsum_yerr = np.cumsum(np.insert(yerr, 0, 0)) \n",
    "        data_trureward_mavecr_yerr = (data_trureward_cumsum_yerr[window_width:] - data_trureward_cumsum_yerr[:-window_width]) / window_width\n",
    "\n",
    "        data_totalreward = np.sum(data_trureward, axis=1) \n",
    "        print(\"total reward length \", len(data_totalreward))\n",
    "        print(\"total reward \", data_totalreward)\n",
    "        data_totalreward_mean = np.mean(data_totalreward)\n",
    "        data_totalreward_std = np.std(data_totalreward)\n",
    "        data_totalreward_yerr = data_totalreward_std / math.sqrt(len(data_totalreward))\n",
    "\n",
    "        reward = {}\n",
    "        reward['total'] = data_totalreward\n",
    "        reward['total_mean'] = data_totalreward_mean\n",
    "        reward['total_std'] = data_totalreward_std\n",
    "        reward['total_yerr'] = data_totalreward_yerr\n",
    "        reward['rwds_last'] = last_elements\n",
    "        reward['mean_hist'] = mean\n",
    "        reward['mean_last'] = last_mean\n",
    "        reward['std_hist'] = std\n",
    "        reward['std_last'] = last_std\n",
    "        reward['yerr_hist'] = yerr\n",
    "        reward['yerr_last'] = last_yerr\n",
    "        reward['mean'] = np.mean(mean)\n",
    "        reward['mavecr'] = data_trureward_mavecr\n",
    "        reward['mavecr_std'] = data_trureward_cumsum_std\n",
    "        reward['mavecr_yerr'] = data_trureward_mavecr_yerr\n",
    "        reward['min_val'] = data_trureward_mavecr - data_trureward_mavecr_yerr\n",
    "        reward['max_val'] = data_trureward_mavecr + data_trureward_mavecr_yerr\n",
    "        reward['length'] = data_trureward_mavecr_std.shape[0]\n",
    "\n",
    "        reward_data[model] = reward\n",
    "\n",
    "save_plot_data(reward_data, 'reward_new_'+task, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from scipy.stats import mannwhitneyu\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_final_rewards(data, models, end_window_size=100):\n",
    "    \"\"\"\n",
    "    Extract final rewards from the last `end_window_size` episodes for each model.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: Dictionary containing reward data for each model.\n",
    "    - models: List of model names to extract.\n",
    "    - end_window_size: Number of episodes to consider from the end.\n",
    "    \n",
    "    Returns:\n",
    "    - final_rewards: Dictionary with final rewards for each model.\n",
    "    \"\"\"\n",
    "    final_rewards = {model: data[model]['abs_tru_reward'][:, -end_window_size:].mean(axis=1) for model in models}\n",
    "    return final_rewards\n",
    "\n",
    "def perform_statistical_tests(final_rewards, models, base_model='SEC'):\n",
    "    \"\"\"\n",
    "    Perform pairwise Mann-Whitney U tests comparing the base_model against other models.\n",
    "    \n",
    "    Parameters:\n",
    "    - final_rewards: Dictionary containing final rewards for each model.\n",
    "    - models: List of model names to compare.\n",
    "    - base_model: The model to compare others against (default: 'SEC').\n",
    "    \n",
    "    Returns:\n",
    "    - results_df: Pandas DataFrame containing the statistical test results.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'Model': [],\n",
    "        'Mean (Base)': [],\n",
    "        'Mean (Comparison)': [],\n",
    "        '95% CI Lower (Base)': [],\n",
    "        '95% CI Upper (Base)': [],\n",
    "        '95% CI Lower (Comparison)': [],\n",
    "        '95% CI Upper (Comparison)': [],\n",
    "        'p-value': [],\n",
    "        'Significant': []\n",
    "    }\n",
    "\n",
    "    # Base model rewards\n",
    "    base_trureward = data[base_model]['abs_tru_reward']\n",
    "    base_trureward = np.asarray(base_trureward, dtype=np.float32)\n",
    "    print(\"base_truerward_sum \", np.sum(base_trureward))\n",
    "\n",
    "    if np.sum(base_trureward) > 0:\n",
    "        if len(base_trureward[0]) > 5000: \n",
    "            based_data_half = np.hsplit(base_trureward, 2)\n",
    "            base_trureward = based_data_half[0]\n",
    "\n",
    "    base_data = base_trureward[:, -1]\n",
    "    base_mean = np.mean(base_data)\n",
    "    base_std = np.std(base_data)\n",
    "    base_sem = base_std / math.sqrt(len(base_data))\n",
    "    base_ci_lower = base_mean - 1.96 * base_sem\n",
    "    base_ci_upper = base_mean + 1.96 * base_sem\n",
    "    \n",
    "    print(\"reward mean last \", base_mean)\n",
    "    print(\"reward std last \", base_std)\n",
    "    print(\"reward sem last\", base_sem)\n",
    "    \n",
    "    for model in models:\n",
    "        if model == base_model:\n",
    "            continue  # Skip comparison with itself\n",
    "\n",
    "        model_trureward = data[model]['abs_tru_reward']\n",
    "        model_trureward = np.asarray(model_trureward, dtype=np.float32)\n",
    "        print(\"model_truerward_sum \", np.sum(model_trureward))\n",
    "\n",
    "        if np.sum(model_trureward) > 0:\n",
    "            if len(model_trureward[0]) > 5000: \n",
    "                model_data_half = np.hsplit(model_trureward, 2)\n",
    "                model_trureward = model_data_half[0]\n",
    "\n",
    "        comp_data  = model_trureward[:, -1]\n",
    "        comp_mean = np.mean(comp_data)\n",
    "        comp_std = np.std(comp_data)\n",
    "        comp_sem = comp_std / math.sqrt(len(comp_data))\n",
    "        comp_ci_lower = comp_mean - 1.96 * comp_sem\n",
    "        comp_ci_upper = comp_mean + 1.96 * comp_sem\n",
    "\n",
    "        print(\"comp reward mean last \", comp_mean)\n",
    "        print(\"comp reward std last \", comp_std)\n",
    "        print(\"comp reward sem last\", comp_sem)\n",
    "        \n",
    "        # Perform Mann-Whitney U test\n",
    "        stat, p_val = mannwhitneyu(base_data, comp_data)\n",
    "\n",
    "        # Determine significance\n",
    "        alpha = 0.05  # Significance level\n",
    "        significant = 'Yes' if p_val < alpha else 'No'\n",
    "\n",
    "        # Populate results\n",
    "        results['Model'].append(model)\n",
    "        results['Mean (Base)'].append(base_mean)\n",
    "        results['Mean (Comparison)'].append(comp_mean)\n",
    "        results['95% CI Lower (Base)'].append(base_ci_lower)\n",
    "        results['95% CI Upper (Base)'].append(base_ci_upper)\n",
    "        results['95% CI Lower (Comparison)'].append(comp_ci_lower)\n",
    "        results['95% CI Upper (Comparison)'].append(comp_ci_upper)\n",
    "        results['p-value'].append(p_val)\n",
    "        results['Significant'].append(significant)\n",
    "\n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "def apply_bonferroni_correction(results_df):\n",
    "    \"\"\"\n",
    "    Apply Bonferroni correction to p-values in the results DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - results_df: DataFrame with statistical test results, including p-values.\n",
    "    \n",
    "    Returns:\n",
    "    - results_df: Updated DataFrame with Bonferroni-adjusted p-values.\n",
    "    \"\"\"\n",
    "    num_comparisons = len(results_df)\n",
    "    results_df['Adjusted p-value (Bonferroni)'] = results_df['p-value'] * num_comparisons\n",
    "    results_df['Adjusted p-value (Bonferroni)'] = results_df['Adjusted p-value (Bonferroni)'].clip(upper=1.0)\n",
    "    return results_df\n",
    "\n",
    "def display_statistical_results(results_df):\n",
    "    \"\"\"\n",
    "    Display the statistical test results in a formatted table.\n",
    "    \n",
    "    Parameters:\n",
    "    - results_df: Pandas DataFrame containing the statistical test results.\n",
    "    \"\"\"\n",
    "    # Format p-values\n",
    "    results_df['p-value'] = results_df['p-value'].apply(lambda x: f\"{x:.4f}\")\n",
    "    \n",
    "    # Display the table\n",
    "    print(\"\\n**Statistical Test Results (Base Model: SEC)**\\n\")\n",
    "    display(results_df.style.hide_index().set_table_styles(\n",
    "        [{\n",
    "            'selector': 'th',\n",
    "            'props': [('font-size', '12pt'), ('text-align', 'center')]\n",
    "        }]\n",
    "    ).set_properties(**{\n",
    "        'font-size': '12pt',\n",
    "        'text-align': 'center'\n",
    "    }))\n",
    "    \n",
    "    # Optionally, save to CSV\n",
    "    # results_df.to_csv('statistical_test_results.csv', index=False)\n",
    "\n",
    "def main_statistical_analysis(data, models):\n",
    "    \"\"\"\n",
    "    Main function to perform extraction of final rewards, statistical testing, and display results.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: Dictionary containing the reward data for each model.\n",
    "    - models: List of model names to compare.\n",
    "    \"\"\"\n",
    "    # Extract final rewards from the last episodes\n",
    "    #final_rewards = extract_final_rewards(data, models, end_window_size=100)\n",
    "\n",
    "    # Perform pairwise statistical tests\n",
    "    results_df = perform_statistical_tests(data, models, base_model='SEC')\n",
    "    \n",
    "    # Apply Bonferroni correction for multiple comparisons\n",
    "    results_df = apply_bonferroni_correction(results_df)\n",
    "    \n",
    "    # Display results\n",
    "    display_statistical_results(results_df)\n",
    "    \n",
    "    # Optional: Save the table as a CSV for inclusion in the manuscript\n",
    "    # results_df.to_csv(plot_path + 'new_statistical_test_results.csv', index=False)\n",
    "    results_df.to_csv(plot_path + task + '_statistical_test_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_truerward_sum  232097.61\n",
      "reward mean last  2.78515\n",
      "reward std last  0.124701716\n",
      "reward sem last 0.027884151389984718\n",
      "model_truerward_sum  100490.64\n",
      "comp reward mean last  1.25415\n",
      "comp reward std last  1.2728102\n",
      "comp reward sem last 0.2846090175982817\n",
      "model_truerward_sum  70509.586\n",
      "comp reward mean last  1.0884\n",
      "comp reward std last  1.33557\n",
      "comp reward sem last 0.2986425258979946\n",
      "model_truerward_sum  22324.096\n",
      "comp reward mean last  0.717\n",
      "comp reward std last  1.2418815\n",
      "comp reward sem last 0.3585003062054628\n",
      "model_truerward_sum  62076.54\n",
      "comp reward mean last  1.9991\n",
      "comp reward std last  1.3089341\n",
      "comp reward sem last 0.4139213039414039\n",
      "model_truerward_sum  78262.09\n",
      "comp reward mean last  1.5838001\n",
      "comp reward std last  1.3031106\n",
      "comp reward sem last 0.4120797537583833\n",
      "\n",
      "**Statistical Test Results (Base Model: SEC)**\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zerat\\AppData\\Local\\Temp\\ipykernel_18988\\305117225.py:141: FutureWarning: this method is deprecated in favour of `Styler.hide(axis='index')`\n",
      "  display(results_df.style.hide_index().set_table_styles(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_38b7d th {\n",
       "  font-size: 12pt;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_38b7d_row0_col0, #T_38b7d_row0_col1, #T_38b7d_row0_col2, #T_38b7d_row0_col3, #T_38b7d_row0_col4, #T_38b7d_row0_col5, #T_38b7d_row0_col6, #T_38b7d_row0_col7, #T_38b7d_row0_col8, #T_38b7d_row0_col9, #T_38b7d_row1_col0, #T_38b7d_row1_col1, #T_38b7d_row1_col2, #T_38b7d_row1_col3, #T_38b7d_row1_col4, #T_38b7d_row1_col5, #T_38b7d_row1_col6, #T_38b7d_row1_col7, #T_38b7d_row1_col8, #T_38b7d_row1_col9, #T_38b7d_row2_col0, #T_38b7d_row2_col1, #T_38b7d_row2_col2, #T_38b7d_row2_col3, #T_38b7d_row2_col4, #T_38b7d_row2_col5, #T_38b7d_row2_col6, #T_38b7d_row2_col7, #T_38b7d_row2_col8, #T_38b7d_row2_col9, #T_38b7d_row3_col0, #T_38b7d_row3_col1, #T_38b7d_row3_col2, #T_38b7d_row3_col3, #T_38b7d_row3_col4, #T_38b7d_row3_col5, #T_38b7d_row3_col6, #T_38b7d_row3_col7, #T_38b7d_row3_col8, #T_38b7d_row3_col9, #T_38b7d_row4_col0, #T_38b7d_row4_col1, #T_38b7d_row4_col2, #T_38b7d_row4_col3, #T_38b7d_row4_col4, #T_38b7d_row4_col5, #T_38b7d_row4_col6, #T_38b7d_row4_col7, #T_38b7d_row4_col8, #T_38b7d_row4_col9 {\n",
       "  font-size: 12pt;\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_38b7d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_38b7d_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_38b7d_level0_col1\" class=\"col_heading level0 col1\" >Mean (Base)</th>\n",
       "      <th id=\"T_38b7d_level0_col2\" class=\"col_heading level0 col2\" >Mean (Comparison)</th>\n",
       "      <th id=\"T_38b7d_level0_col3\" class=\"col_heading level0 col3\" >95% CI Lower (Base)</th>\n",
       "      <th id=\"T_38b7d_level0_col4\" class=\"col_heading level0 col4\" >95% CI Upper (Base)</th>\n",
       "      <th id=\"T_38b7d_level0_col5\" class=\"col_heading level0 col5\" >95% CI Lower (Comparison)</th>\n",
       "      <th id=\"T_38b7d_level0_col6\" class=\"col_heading level0 col6\" >95% CI Upper (Comparison)</th>\n",
       "      <th id=\"T_38b7d_level0_col7\" class=\"col_heading level0 col7\" >p-value</th>\n",
       "      <th id=\"T_38b7d_level0_col8\" class=\"col_heading level0 col8\" >Significant</th>\n",
       "      <th id=\"T_38b7d_level0_col9\" class=\"col_heading level0 col9\" >Adjusted p-value (Bonferroni)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_38b7d_row0_col0\" class=\"data row0 col0\" >NSEC</td>\n",
       "      <td id=\"T_38b7d_row0_col1\" class=\"data row0 col1\" >2.785150</td>\n",
       "      <td id=\"T_38b7d_row0_col2\" class=\"data row0 col2\" >1.254150</td>\n",
       "      <td id=\"T_38b7d_row0_col3\" class=\"data row0 col3\" >2.730497</td>\n",
       "      <td id=\"T_38b7d_row0_col4\" class=\"data row0 col4\" >2.839803</td>\n",
       "      <td id=\"T_38b7d_row0_col5\" class=\"data row0 col5\" >0.696316</td>\n",
       "      <td id=\"T_38b7d_row0_col6\" class=\"data row0 col6\" >1.811984</td>\n",
       "      <td id=\"T_38b7d_row0_col7\" class=\"data row0 col7\" >0.0000</td>\n",
       "      <td id=\"T_38b7d_row0_col8\" class=\"data row0 col8\" >Yes</td>\n",
       "      <td id=\"T_38b7d_row0_col9\" class=\"data row0 col9\" >0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_38b7d_row1_col0\" class=\"data row1 col0\" >MFEC-ae</td>\n",
       "      <td id=\"T_38b7d_row1_col1\" class=\"data row1 col1\" >2.785150</td>\n",
       "      <td id=\"T_38b7d_row1_col2\" class=\"data row1 col2\" >1.088400</td>\n",
       "      <td id=\"T_38b7d_row1_col3\" class=\"data row1 col3\" >2.730497</td>\n",
       "      <td id=\"T_38b7d_row1_col4\" class=\"data row1 col4\" >2.839803</td>\n",
       "      <td id=\"T_38b7d_row1_col5\" class=\"data row1 col5\" >0.503061</td>\n",
       "      <td id=\"T_38b7d_row1_col6\" class=\"data row1 col6\" >1.673739</td>\n",
       "      <td id=\"T_38b7d_row1_col7\" class=\"data row1 col7\" >0.0000</td>\n",
       "      <td id=\"T_38b7d_row1_col8\" class=\"data row1 col8\" >Yes</td>\n",
       "      <td id=\"T_38b7d_row1_col9\" class=\"data row1 col9\" >0.000163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_38b7d_row2_col0\" class=\"data row2 col0\" >MFEC-rp</td>\n",
       "      <td id=\"T_38b7d_row2_col1\" class=\"data row2 col1\" >2.785150</td>\n",
       "      <td id=\"T_38b7d_row2_col2\" class=\"data row2 col2\" >0.717000</td>\n",
       "      <td id=\"T_38b7d_row2_col3\" class=\"data row2 col3\" >2.730497</td>\n",
       "      <td id=\"T_38b7d_row2_col4\" class=\"data row2 col4\" >2.839803</td>\n",
       "      <td id=\"T_38b7d_row2_col5\" class=\"data row2 col5\" >0.014339</td>\n",
       "      <td id=\"T_38b7d_row2_col6\" class=\"data row2 col6\" >1.419661</td>\n",
       "      <td id=\"T_38b7d_row2_col7\" class=\"data row2 col7\" >0.0117</td>\n",
       "      <td id=\"T_38b7d_row2_col8\" class=\"data row2 col8\" >Yes</td>\n",
       "      <td id=\"T_38b7d_row2_col9\" class=\"data row2 col9\" >0.058678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_38b7d_row3_col0\" class=\"data row3 col0\" >DQN</td>\n",
       "      <td id=\"T_38b7d_row3_col1\" class=\"data row3 col1\" >2.785150</td>\n",
       "      <td id=\"T_38b7d_row3_col2\" class=\"data row3 col2\" >1.999100</td>\n",
       "      <td id=\"T_38b7d_row3_col3\" class=\"data row3 col3\" >2.730497</td>\n",
       "      <td id=\"T_38b7d_row3_col4\" class=\"data row3 col4\" >2.839803</td>\n",
       "      <td id=\"T_38b7d_row3_col5\" class=\"data row3 col5\" >1.187814</td>\n",
       "      <td id=\"T_38b7d_row3_col6\" class=\"data row3 col6\" >2.810386</td>\n",
       "      <td id=\"T_38b7d_row3_col7\" class=\"data row3 col7\" >0.6281</td>\n",
       "      <td id=\"T_38b7d_row3_col8\" class=\"data row3 col8\" >No</td>\n",
       "      <td id=\"T_38b7d_row3_col9\" class=\"data row3 col9\" >1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_38b7d_row4_col0\" class=\"data row4 col0\" >ERLAM</td>\n",
       "      <td id=\"T_38b7d_row4_col1\" class=\"data row4 col1\" >2.785150</td>\n",
       "      <td id=\"T_38b7d_row4_col2\" class=\"data row4 col2\" >1.583800</td>\n",
       "      <td id=\"T_38b7d_row4_col3\" class=\"data row4 col3\" >2.730497</td>\n",
       "      <td id=\"T_38b7d_row4_col4\" class=\"data row4 col4\" >2.839803</td>\n",
       "      <td id=\"T_38b7d_row4_col5\" class=\"data row4 col5\" >0.776124</td>\n",
       "      <td id=\"T_38b7d_row4_col6\" class=\"data row4 col6\" >2.391476</td>\n",
       "      <td id=\"T_38b7d_row4_col7\" class=\"data row4 col7\" >0.0077</td>\n",
       "      <td id=\"T_38b7d_row4_col8\" class=\"data row4 col8\" >Yes</td>\n",
       "      <td id=\"T_38b7d_row4_col9\" class=\"data row4 col9\" >0.038437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1b6eb485070>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage:\n",
    "task = 'DoubleTMaze'\n",
    "\n",
    "# Define file paths\n",
    "file_path = '../data/exp1_2tmaze/'\n",
    "plot_path = '../data/exp1_2tmaze/'\n",
    "\n",
    "# Load data\n",
    "data = load_plot_data('data_v3', file_path)  # Ensure this function returns the correct structure\n",
    "\n",
    "# Define models and labels\n",
    "models = ['SEC', 'NSEC', 'MFEC-ae', 'MFEC-rp', 'DQN', 'ERLAM']\n",
    "#labels = ['SEC', 'NSEC', 'MFEC-ae', 'MFEC-rp', 'DQN', 'ERLAM']\n",
    "    \n",
    "main_statistical_analysis(data, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from scipy.stats import mannwhitneyu\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_final_rewards(data, models, end_window_size=100):\n",
    "    \"\"\"\n",
    "    Extract final rewards from the last `end_window_size` episodes for each model.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: Dictionary containing reward data for each model.\n",
    "    - models: List of model names to extract.\n",
    "    - end_window_size: Number of episodes to consider from the end.\n",
    "    \n",
    "    Returns:\n",
    "    - final_rewards: Dictionary with final rewards for each model.\n",
    "    \"\"\"\n",
    "    final_rewards = {model: data[model]['abs_tru_reward'][:, -end_window_size:].mean(axis=1) for model in models}\n",
    "    return final_rewards\n",
    "\n",
    "def perform_statistical_tests(final_rewards, models, base_model='SEC'):\n",
    "    \"\"\"\n",
    "    Perform pairwise Mann-Whitney U tests comparing the base_model against other models.\n",
    "    \n",
    "    Parameters:\n",
    "    - final_rewards: Dictionary containing final rewards for each model.\n",
    "    - models: List of model names to compare.\n",
    "    - base_model: The model to compare others against (default: 'SEC').\n",
    "    \n",
    "    Returns:\n",
    "    - results_df: Pandas DataFrame containing the statistical test results.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'Model': [],\n",
    "        'Mean': [],\n",
    "        '95% CI Lower': [],\n",
    "        '95% CI Upper': [],\n",
    "        'p-value': [],\n",
    "        'Significant': []\n",
    "    }\n",
    "\n",
    "    # Base model rewards\n",
    "    base_trureward = data[base_model]['abs_tru_reward']\n",
    "    base_trureward = np.asarray(base_trureward, dtype=np.float32)\n",
    "    print(\"base_truerward_sum \", np.sum(base_trureward))\n",
    "\n",
    "    if np.sum(base_trureward) > 0:\n",
    "        if len(base_trureward[0]) > 5000: \n",
    "            based_data_half = np.hsplit(base_trureward, 2)\n",
    "            base_trureward = based_data_half[0]\n",
    "\n",
    "    base_data = base_trureward[:, -1]\n",
    "    base_mean = np.mean(base_data)\n",
    "    base_std = np.std(base_data)\n",
    "    base_sem = base_std / math.sqrt(len(base_data))\n",
    "    base_ci_lower = base_mean - 1.96 * base_sem\n",
    "    base_ci_upper = base_mean + 1.96 * base_sem\n",
    "    \n",
    "    print(\"reward mean last \", base_mean)\n",
    "    print(\"reward std last \", base_std)\n",
    "    print(\"reward sem last\", base_sem)\n",
    "    \n",
    "    results['Model'].append(base_model)\n",
    "    results['Mean'].append(base_mean)\n",
    "    results['95% CI Lower'].append(base_ci_lower)\n",
    "    results['95% CI Upper'].append(base_ci_upper)\n",
    "    results['p-value'].append(None)  # No p-value for the base model\n",
    "    results['Significant'].append(None)  # No significance comparison for the base model\n",
    "    \n",
    "    for model in models:\n",
    "        if model == base_model:\n",
    "            continue  # Skip comparison with itself\n",
    "\n",
    "        model_trureward = data[model]['abs_tru_reward']\n",
    "        model_trureward = np.asarray(model_trureward, dtype=np.float32)\n",
    "        print(\"model_truerward_sum \", np.sum(model_trureward))\n",
    "\n",
    "        if np.sum(model_trureward) > 0:\n",
    "            if len(model_trureward[0]) > 5000: \n",
    "                model_data_half = np.hsplit(model_trureward, 2)\n",
    "                model_trureward = model_data_half[0]\n",
    "\n",
    "        comp_data  = model_trureward[:, -1]\n",
    "        comp_mean = np.mean(comp_data)\n",
    "        comp_std = np.std(comp_data)\n",
    "        comp_sem = comp_std / math.sqrt(len(comp_data))\n",
    "        comp_ci_lower = comp_mean - 1.96 * comp_sem\n",
    "        comp_ci_upper = comp_mean + 1.96 * comp_sem\n",
    "\n",
    "        print(\"comp reward mean last \", comp_mean)\n",
    "        print(\"comp reward std last \", comp_std)\n",
    "        print(\"comp reward sem last\", comp_sem)\n",
    "        \n",
    "        # Perform Mann-Whitney U test\n",
    "        stat, p_val = mannwhitneyu(base_data, comp_data)\n",
    "\n",
    "        # Determine significance\n",
    "        alpha = 0.05  # Significance level\n",
    "        significant = 'Yes' if p_val < alpha else 'No'\n",
    "\n",
    "        # Populate results\n",
    "        results['Model'].append(model)\n",
    "        results['Mean'].append(comp_mean)\n",
    "        results['95% CI Lower'].append(comp_ci_lower)\n",
    "        results['95% CI Upper'].append(comp_ci_upper)\n",
    "        results['p-value'].append(p_val)\n",
    "        results['Significant'].append(significant)\n",
    "        \n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "def apply_bonferroni_correction(results_df):\n",
    "    \"\"\"\n",
    "    Apply Bonferroni correction to p-values in the results DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - results_df: DataFrame with statistical test results, including p-values.\n",
    "    \n",
    "    Returns:\n",
    "    - results_df: Updated DataFrame with Bonferroni-adjusted p-values.\n",
    "    \"\"\"\n",
    "    num_comparisons = len(results_df)\n",
    "    results_df['Adjusted p-value (Bonferroni)'] = results_df['p-value'] * num_comparisons\n",
    "    results_df['Adjusted p-value (Bonferroni)'] = results_df['Adjusted p-value (Bonferroni)'].clip(upper=1.0)\n",
    "    return results_df\n",
    "\n",
    "def display_statistical_results(results_df):\n",
    "    \"\"\"\n",
    "    Display the statistical test results in a formatted table.\n",
    "    \n",
    "    Parameters:\n",
    "    - results_df: Pandas DataFrame containing the statistical test results.\n",
    "    \"\"\"\n",
    "    # Format p-values\n",
    "    results_df['p-value'] = results_df['p-value'].apply(lambda x: f\"{x:.4f}\" if pd.notna(x) else 'N/A')\n",
    "    \n",
    "    # Display the table\n",
    "    print(\"\\n**Statistical Test Results (Base Model: SEC)**\\n\")\n",
    "    display(results_df.style.hide_index().set_table_styles(\n",
    "        [{\n",
    "            'selector': 'th',\n",
    "            'props': [('font-size', '12pt'), ('text-align', 'center')]\n",
    "        }]\n",
    "    ).set_properties(**{\n",
    "        'font-size': '12pt',\n",
    "        'text-align': 'center'\n",
    "    }))\n",
    "    \n",
    "    # Optionally, save to CSV\n",
    "    # results_df.to_csv('statistical_test_results.csv', index=False)\n",
    "\n",
    "def main_statistical_analysis(data, models, base_model='SEC'):\n",
    "    \"\"\"\n",
    "    Main function to perform extraction of final rewards, statistical testing, and display results.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: Dictionary containing the reward data for each model.\n",
    "    - models: List of model names to compare.\n",
    "    \"\"\"\n",
    "    # Extract final rewards from the last episodes\n",
    "    #final_rewards = extract_final_rewards(data, models, end_window_size=100)\n",
    "\n",
    "    # Perform pairwise statistical tests\n",
    "    results_df = perform_statistical_tests(data, models, base_model=base_model)\n",
    "    \n",
    "    # Apply Bonferroni correction for multiple comparisons\n",
    "    results_df = apply_bonferroni_correction(results_df)\n",
    "    \n",
    "    # Display results\n",
    "    display_statistical_results(results_df)\n",
    "    \n",
    "    # Optional: Save the table as a CSV for inclusion in the manuscript\n",
    "    results_df.to_csv(plot_path + task + '_statistical_test_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from scipy.stats import mannwhitneyu\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_final_rewards(data, models, end_window_size=100):\n",
    "    \"\"\"\n",
    "    Extract final rewards from the last `end_window_size` episodes for each model.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: Dictionary containing reward data for each model.\n",
    "    - models: List of model names to extract.\n",
    "    - end_window_size: Number of episodes to consider from the end.\n",
    "    \n",
    "    Returns:\n",
    "    - final_rewards: Dictionary with final rewards for each model.\n",
    "    \"\"\"\n",
    "    final_rewards = {model: data[model]['abs_tru_reward'][:, -end_window_size:].mean(axis=1) for model in models}\n",
    "    return final_rewards\n",
    "\n",
    "def perform_statistical_tests(final_rewards, models, base_model='SEC'):\n",
    "    \"\"\"\n",
    "    Perform pairwise Mann-Whitney U tests comparing the base_model against other models.\n",
    "    \n",
    "    Parameters:\n",
    "    - final_rewards: Dictionary containing final rewards for each model.\n",
    "    - models: List of model names to compare.\n",
    "    - base_model: The model to compare others against (default: 'SEC').\n",
    "    \n",
    "    Returns:\n",
    "    - results_df: Pandas DataFrame containing the statistical test results.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'Model': [],\n",
    "        'Mean': [],\n",
    "        '95% CI Lower': [],\n",
    "        '95% CI Upper': [],\n",
    "        'p-value': [],\n",
    "        'Significant': []\n",
    "    }\n",
    "\n",
    "    # Base model rewards\n",
    "    base_trureward = data[base_model]['abs_tru_reward']\n",
    "    base_trureward = np.asarray(base_trureward, dtype=np.float32)\n",
    "    print(\"base_truerward_sum \", np.sum(base_trureward))\n",
    "\n",
    "    if np.sum(base_trureward) > 0:\n",
    "        if len(base_trureward[0]) > 5000: \n",
    "            based_data_half = np.hsplit(base_trureward, 2)\n",
    "            base_trureward = based_data_half[0]\n",
    "\n",
    "    base_data = np.sum(base_trureward, axis=1)\n",
    "    base_mean = np.mean(base_data)\n",
    "    base_std = np.std(base_data)\n",
    "    base_sem = base_std / math.sqrt(len(base_data))\n",
    "    base_ci_lower = base_mean - 1.96 * base_sem\n",
    "    base_ci_upper = base_mean + 1.96 * base_sem\n",
    "    \n",
    "    print(\"total reward mean \", base_mean)\n",
    "    print(\"total reward std \", base_std)\n",
    "    print(\"total reward sem\", base_sem)\n",
    "    \n",
    "    results['Model'].append(base_model)\n",
    "    results['Mean'].append(base_mean)\n",
    "    results['95% CI Lower'].append(base_ci_lower)\n",
    "    results['95% CI Upper'].append(base_ci_upper)\n",
    "    results['p-value'].append(None)  # No p-value for the base model\n",
    "    results['Significant'].append(None)  # No significance comparison for the base model\n",
    "    \n",
    "    for model in models:\n",
    "        if model == base_model:\n",
    "            continue  # Skip comparison with itself\n",
    "\n",
    "        model_trureward = data[model]['abs_tru_reward']\n",
    "        model_trureward = np.asarray(model_trureward, dtype=np.float32)\n",
    "        print(\"model_truerward_sum \", np.sum(model_trureward))\n",
    "\n",
    "        if np.sum(model_trureward) > 0:\n",
    "            if len(model_trureward[0]) > 5000: \n",
    "                model_data_half = np.hsplit(model_trureward, 2)\n",
    "                model_trureward = model_data_half[0]\n",
    "\n",
    "        comp_data = np.sum(model_trureward, axis=1)\n",
    "        comp_mean = np.mean(comp_data)\n",
    "        comp_std = np.std(comp_data)\n",
    "        comp_sem = comp_std / math.sqrt(len(comp_data))\n",
    "        comp_ci_lower = comp_mean - 1.96 * comp_sem\n",
    "        comp_ci_upper = comp_mean + 1.96 * comp_sem\n",
    "\n",
    "        print(\"total comp reward mean \", comp_mean)\n",
    "        print(\"total comp reward std \", comp_std)\n",
    "        print(\"total comp reward sem\", comp_sem)\n",
    "        \n",
    "        # Perform Mann-Whitney U test\n",
    "        stat, p_val = mannwhitneyu(base_data, comp_data)\n",
    "\n",
    "        # Determine significance\n",
    "        alpha = 0.05  # Significance level\n",
    "        significant = 'Yes' if p_val < alpha else 'No'\n",
    "\n",
    "        # Populate results\n",
    "        results['Model'].append(model)\n",
    "        results['Mean'].append(comp_mean)\n",
    "        results['95% CI Lower'].append(comp_ci_lower)\n",
    "        results['95% CI Upper'].append(comp_ci_upper)\n",
    "        results['p-value'].append(p_val)\n",
    "        results['Significant'].append(significant)\n",
    "        \n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "def apply_bonferroni_correction(results_df):\n",
    "    \"\"\"\n",
    "    Apply Bonferroni correction to p-values in the results DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - results_df: DataFrame with statistical test results, including p-values.\n",
    "    \n",
    "    Returns:\n",
    "    - results_df: Updated DataFrame with Bonferroni-adjusted p-values.\n",
    "    \"\"\"\n",
    "    num_comparisons = len(results_df)\n",
    "    results_df['Adjusted p-value (Bonferroni)'] = results_df['p-value'] * num_comparisons\n",
    "    results_df['Adjusted p-value (Bonferroni)'] = results_df['Adjusted p-value (Bonferroni)'].clip(upper=1.0)\n",
    "    return results_df\n",
    "\n",
    "def display_statistical_results(results_df):\n",
    "    \"\"\"\n",
    "    Display the statistical test results in a formatted table.\n",
    "    \n",
    "    Parameters:\n",
    "    - results_df: Pandas DataFrame containing the statistical test results.\n",
    "    \"\"\"\n",
    "    # Format p-values\n",
    "    results_df['p-value'] = results_df['p-value'].apply(lambda x: f\"{x:.4f}\" if pd.notna(x) else 'N/A')\n",
    "    \n",
    "    # Display the table\n",
    "    print(\"\\n**Statistical Test Results (Base Model: SEC)**\\n\")\n",
    "    display(results_df.style.hide_index().set_table_styles(\n",
    "        [{\n",
    "            'selector': 'th',\n",
    "            'props': [('font-size', '12pt'), ('text-align', 'center')]\n",
    "        }]\n",
    "    ).set_properties(**{\n",
    "        'font-size': '12pt',\n",
    "        'text-align': 'center'\n",
    "    }))\n",
    "    \n",
    "    # Optionally, save to CSV\n",
    "    # results_df.to_csv('statistical_test_results.csv', index=False)\n",
    "\n",
    "def main_statistical_analysis(data, models, base_model='SEC'):\n",
    "    \"\"\"\n",
    "    Main function to perform extraction of final rewards, statistical testing, and display results.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: Dictionary containing the reward data for each model.\n",
    "    - models: List of model names to compare.\n",
    "    \"\"\"\n",
    "    # Extract final rewards from the last episodes\n",
    "    #final_rewards = extract_final_rewards(data, models, end_window_size=100)\n",
    "\n",
    "    # Perform pairwise statistical tests\n",
    "    results_df = perform_statistical_tests(data, models, base_model=base_model)\n",
    "    \n",
    "    # Apply Bonferroni correction for multiple comparisons\n",
    "    results_df = apply_bonferroni_correction(results_df)\n",
    "    \n",
    "    # Display results\n",
    "    display_statistical_results(results_df)\n",
    "    \n",
    "    # Optional: Save the table as a CSV for inclusion in the manuscript\n",
    "    results_df.to_csv(plot_path + task + '_statistical_test_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For exp 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_truerward_sum  232097.61\n",
      "total reward mean  11604.881\n",
      "total reward std  1649.4579\n",
      "total reward sem 368.8299958542612\n",
      "model_truerward_sum  100490.64\n",
      "total comp reward mean  5024.5312\n",
      "total comp reward std  2889.6023\n",
      "total comp reward sem 646.1347159384708\n",
      "model_truerward_sum  70509.586\n",
      "total comp reward mean  3525.4792\n",
      "total comp reward std  2808.5393\n",
      "total comp reward sem 628.0084807128563\n",
      "model_truerward_sum  22324.096\n",
      "total comp reward mean  1860.3412\n",
      "total comp reward std  3041.2249\n",
      "total comp reward sem 877.9259939217131\n",
      "model_truerward_sum  62076.54\n",
      "total comp reward mean  6207.6543\n",
      "total comp reward std  4202.738\n",
      "total comp reward sem 1329.0223834250437\n",
      "model_truerward_sum  78262.09\n",
      "total comp reward mean  7826.2095\n",
      "total comp reward std  2651.4067\n",
      "total comp reward sem 838.4484296486705\n",
      "\n",
      "**Statistical Test Results (Base Model: SEC)**\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zerat\\AppData\\Local\\Temp\\ipykernel_18988\\2656814379.py:142: FutureWarning: this method is deprecated in favour of `Styler.hide(axis='index')`\n",
      "  display(results_df.style.hide_index().set_table_styles(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3b5ce th {\n",
       "  font-size: 12pt;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_3b5ce_row0_col0, #T_3b5ce_row0_col1, #T_3b5ce_row0_col2, #T_3b5ce_row0_col3, #T_3b5ce_row0_col4, #T_3b5ce_row0_col5, #T_3b5ce_row0_col6, #T_3b5ce_row1_col0, #T_3b5ce_row1_col1, #T_3b5ce_row1_col2, #T_3b5ce_row1_col3, #T_3b5ce_row1_col4, #T_3b5ce_row1_col5, #T_3b5ce_row1_col6, #T_3b5ce_row2_col0, #T_3b5ce_row2_col1, #T_3b5ce_row2_col2, #T_3b5ce_row2_col3, #T_3b5ce_row2_col4, #T_3b5ce_row2_col5, #T_3b5ce_row2_col6, #T_3b5ce_row3_col0, #T_3b5ce_row3_col1, #T_3b5ce_row3_col2, #T_3b5ce_row3_col3, #T_3b5ce_row3_col4, #T_3b5ce_row3_col5, #T_3b5ce_row3_col6, #T_3b5ce_row4_col0, #T_3b5ce_row4_col1, #T_3b5ce_row4_col2, #T_3b5ce_row4_col3, #T_3b5ce_row4_col4, #T_3b5ce_row4_col5, #T_3b5ce_row4_col6, #T_3b5ce_row5_col0, #T_3b5ce_row5_col1, #T_3b5ce_row5_col2, #T_3b5ce_row5_col3, #T_3b5ce_row5_col4, #T_3b5ce_row5_col5, #T_3b5ce_row5_col6 {\n",
       "  font-size: 12pt;\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3b5ce\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_3b5ce_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_3b5ce_level0_col1\" class=\"col_heading level0 col1\" >Mean</th>\n",
       "      <th id=\"T_3b5ce_level0_col2\" class=\"col_heading level0 col2\" >95% CI Lower</th>\n",
       "      <th id=\"T_3b5ce_level0_col3\" class=\"col_heading level0 col3\" >95% CI Upper</th>\n",
       "      <th id=\"T_3b5ce_level0_col4\" class=\"col_heading level0 col4\" >p-value</th>\n",
       "      <th id=\"T_3b5ce_level0_col5\" class=\"col_heading level0 col5\" >Significant</th>\n",
       "      <th id=\"T_3b5ce_level0_col6\" class=\"col_heading level0 col6\" >Adjusted p-value (Bonferroni)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_3b5ce_row0_col0\" class=\"data row0 col0\" >SEC</td>\n",
       "      <td id=\"T_3b5ce_row0_col1\" class=\"data row0 col1\" >11604.880859</td>\n",
       "      <td id=\"T_3b5ce_row0_col2\" class=\"data row0 col2\" >10881.974068</td>\n",
       "      <td id=\"T_3b5ce_row0_col3\" class=\"data row0 col3\" >12327.787651</td>\n",
       "      <td id=\"T_3b5ce_row0_col4\" class=\"data row0 col4\" >N/A</td>\n",
       "      <td id=\"T_3b5ce_row0_col5\" class=\"data row0 col5\" >None</td>\n",
       "      <td id=\"T_3b5ce_row0_col6\" class=\"data row0 col6\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3b5ce_row1_col0\" class=\"data row1 col0\" >NSEC</td>\n",
       "      <td id=\"T_3b5ce_row1_col1\" class=\"data row1 col1\" >5024.531250</td>\n",
       "      <td id=\"T_3b5ce_row1_col2\" class=\"data row1 col2\" >3758.107207</td>\n",
       "      <td id=\"T_3b5ce_row1_col3\" class=\"data row1 col3\" >6290.955293</td>\n",
       "      <td id=\"T_3b5ce_row1_col4\" class=\"data row1 col4\" >0.0000</td>\n",
       "      <td id=\"T_3b5ce_row1_col5\" class=\"data row1 col5\" >Yes</td>\n",
       "      <td id=\"T_3b5ce_row1_col6\" class=\"data row1 col6\" >0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3b5ce_row2_col0\" class=\"data row2 col0\" >MFEC-ae</td>\n",
       "      <td id=\"T_3b5ce_row2_col1\" class=\"data row2 col1\" >3525.479248</td>\n",
       "      <td id=\"T_3b5ce_row2_col2\" class=\"data row2 col2\" >2294.582626</td>\n",
       "      <td id=\"T_3b5ce_row2_col3\" class=\"data row2 col3\" >4756.375870</td>\n",
       "      <td id=\"T_3b5ce_row2_col4\" class=\"data row2 col4\" >0.0000</td>\n",
       "      <td id=\"T_3b5ce_row2_col5\" class=\"data row2 col5\" >Yes</td>\n",
       "      <td id=\"T_3b5ce_row2_col6\" class=\"data row2 col6\" >0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3b5ce_row3_col0\" class=\"data row3 col0\" >MFEC-rp</td>\n",
       "      <td id=\"T_3b5ce_row3_col1\" class=\"data row3 col1\" >1860.341187</td>\n",
       "      <td id=\"T_3b5ce_row3_col2\" class=\"data row3 col2\" >139.606238</td>\n",
       "      <td id=\"T_3b5ce_row3_col3\" class=\"data row3 col3\" >3581.076135</td>\n",
       "      <td id=\"T_3b5ce_row3_col4\" class=\"data row3 col4\" >0.0000</td>\n",
       "      <td id=\"T_3b5ce_row3_col5\" class=\"data row3 col5\" >Yes</td>\n",
       "      <td id=\"T_3b5ce_row3_col6\" class=\"data row3 col6\" >0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3b5ce_row4_col0\" class=\"data row4 col0\" >DQN</td>\n",
       "      <td id=\"T_3b5ce_row4_col1\" class=\"data row4 col1\" >6207.654297</td>\n",
       "      <td id=\"T_3b5ce_row4_col2\" class=\"data row4 col2\" >3602.770425</td>\n",
       "      <td id=\"T_3b5ce_row4_col3\" class=\"data row4 col3\" >8812.538168</td>\n",
       "      <td id=\"T_3b5ce_row4_col4\" class=\"data row4 col4\" >0.0002</td>\n",
       "      <td id=\"T_3b5ce_row4_col5\" class=\"data row4 col5\" >Yes</td>\n",
       "      <td id=\"T_3b5ce_row4_col6\" class=\"data row4 col6\" >0.001207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3b5ce_row5_col0\" class=\"data row5 col0\" >ERLAM</td>\n",
       "      <td id=\"T_3b5ce_row5_col1\" class=\"data row5 col1\" >7826.209473</td>\n",
       "      <td id=\"T_3b5ce_row5_col2\" class=\"data row5 col2\" >6182.850551</td>\n",
       "      <td id=\"T_3b5ce_row5_col3\" class=\"data row5 col3\" >9469.568395</td>\n",
       "      <td id=\"T_3b5ce_row5_col4\" class=\"data row5 col4\" >0.0002</td>\n",
       "      <td id=\"T_3b5ce_row5_col5\" class=\"data row5 col5\" >Yes</td>\n",
       "      <td id=\"T_3b5ce_row5_col6\" class=\"data row5 col6\" >0.001435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1b6c1aaadf0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage:\n",
    "task = 'DoubleTMaze'\n",
    "\n",
    "# Define file paths\n",
    "file_path = '../data/exp1_2tmaze/'\n",
    "plot_path = '../data/exp1_2tmaze/'\n",
    "\n",
    "# Load data\n",
    "data = load_plot_data('data_v3', file_path)  # Ensure this function returns the correct structure\n",
    "data.keys()\n",
    "\n",
    "# Define models and labels\n",
    "models_tmaze = ['SEC', 'NSEC', 'MFEC-ae', 'MFEC-rp', 'DQN', 'ERLAM']\n",
    "models_cylinder = ['SEC_500', 'NSEC_500', 'MFEC_VAE_100K', 'MFEC_RP_100K', 'DQN_25k', 'ERLAM_5k']\n",
    "models_detour = ['SEC_500', 'NSEC_500', 'MFEC_VAE_100K', 'MFEC_RP_100K', 'DQN_25k', 'ERLAM_25k']\n",
    "models_permanence = ['SEC_500', 'NSEC_500', 'MFEC_VAE_100K', 'MFEC_RP_100K', 'DQN_25k', 'ERLAM_5k']\n",
    "#labels = ['SEC', 'NSEC', 'MFEC-ae', 'MFEC-rp', 'DQN', 'ERLAM']\n",
    "    \n",
    "main_statistical_analysis(data, models_tmaze, base_model='SEC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For exp 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['SEC_ltm125', 'SEC_ltm250', 'SEC_ltm500', 'SEC_ltm1000', 'NSEC_ltm125', 'NSEC_ltm250', 'NSEC_ltm500', 'NSEC_ltm1000'])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage:\n",
    "task = 'Memory Constraints'\n",
    "\n",
    "# Define file paths\n",
    "file_path = '../data/exp2_memory_constraints/'\n",
    "plot_path = '../data/exp2_memory_constraints/'\n",
    "\n",
    "# Load data\n",
    "data = load_plot_data('data_true', file_path)  # Ensure this function returns the correct structure\n",
    "data.keys()\n",
    "\n",
    "\n",
    "# Define models and labels\n",
    "#models_SEC = ['SEC_ltm125', 'SEC_ltm250', 'SEC_ltm500', 'SEC_ltm1000']\n",
    "#models_NSEC = ['NSEC_ltm125', 'NSEC_ltm250', 'NSEC_ltm500', 'NSEC_ltm1000']\n",
    "    \n",
    "#main_statistical_analysis(data, models_SEC, base_model='SEC_ltm1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_truerward_sum  513458.06\n",
      "total reward mean  12174.311\n",
      "total reward std  843.6874\n",
      "total reward sem 188.6542328809337\n",
      "model_truerward_sum  184826.64\n",
      "total comp reward mean  9241.334\n",
      "total comp reward std  1812.4886\n",
      "total comp reward sem 405.28478241693074\n",
      "model_truerward_sum  207027.78\n",
      "total comp reward mean  10351.388\n",
      "total comp reward std  2054.2239\n",
      "total comp reward sem 459.3384229870351\n",
      "model_truerward_sum  500778.75\n",
      "total comp reward mean  12031.594\n",
      "total comp reward std  1303.3646\n",
      "total comp reward sem 291.44118987848617\n",
      "\n",
      "**Statistical Test Results (Base Model: SEC)**\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zerat\\AppData\\Local\\Temp\\ipykernel_18988\\2656814379.py:142: FutureWarning: this method is deprecated in favour of `Styler.hide(axis='index')`\n",
      "  display(results_df.style.hide_index().set_table_styles(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_0f3d6 th {\n",
       "  font-size: 12pt;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_0f3d6_row0_col0, #T_0f3d6_row0_col1, #T_0f3d6_row0_col2, #T_0f3d6_row0_col3, #T_0f3d6_row0_col4, #T_0f3d6_row0_col5, #T_0f3d6_row0_col6, #T_0f3d6_row1_col0, #T_0f3d6_row1_col1, #T_0f3d6_row1_col2, #T_0f3d6_row1_col3, #T_0f3d6_row1_col4, #T_0f3d6_row1_col5, #T_0f3d6_row1_col6, #T_0f3d6_row2_col0, #T_0f3d6_row2_col1, #T_0f3d6_row2_col2, #T_0f3d6_row2_col3, #T_0f3d6_row2_col4, #T_0f3d6_row2_col5, #T_0f3d6_row2_col6, #T_0f3d6_row3_col0, #T_0f3d6_row3_col1, #T_0f3d6_row3_col2, #T_0f3d6_row3_col3, #T_0f3d6_row3_col4, #T_0f3d6_row3_col5, #T_0f3d6_row3_col6 {\n",
       "  font-size: 12pt;\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_0f3d6\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_0f3d6_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_0f3d6_level0_col1\" class=\"col_heading level0 col1\" >Mean</th>\n",
       "      <th id=\"T_0f3d6_level0_col2\" class=\"col_heading level0 col2\" >95% CI Lower</th>\n",
       "      <th id=\"T_0f3d6_level0_col3\" class=\"col_heading level0 col3\" >95% CI Upper</th>\n",
       "      <th id=\"T_0f3d6_level0_col4\" class=\"col_heading level0 col4\" >p-value</th>\n",
       "      <th id=\"T_0f3d6_level0_col5\" class=\"col_heading level0 col5\" >Significant</th>\n",
       "      <th id=\"T_0f3d6_level0_col6\" class=\"col_heading level0 col6\" >Adjusted p-value (Bonferroni)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_0f3d6_row0_col0\" class=\"data row0 col0\" >SEC_ltm1000</td>\n",
       "      <td id=\"T_0f3d6_row0_col1\" class=\"data row0 col1\" >12174.310547</td>\n",
       "      <td id=\"T_0f3d6_row0_col2\" class=\"data row0 col2\" >11804.548250</td>\n",
       "      <td id=\"T_0f3d6_row0_col3\" class=\"data row0 col3\" >12544.072843</td>\n",
       "      <td id=\"T_0f3d6_row0_col4\" class=\"data row0 col4\" >N/A</td>\n",
       "      <td id=\"T_0f3d6_row0_col5\" class=\"data row0 col5\" >None</td>\n",
       "      <td id=\"T_0f3d6_row0_col6\" class=\"data row0 col6\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_0f3d6_row1_col0\" class=\"data row1 col0\" >SEC_ltm125</td>\n",
       "      <td id=\"T_0f3d6_row1_col1\" class=\"data row1 col1\" >9241.333984</td>\n",
       "      <td id=\"T_0f3d6_row1_col2\" class=\"data row1 col2\" >8446.975811</td>\n",
       "      <td id=\"T_0f3d6_row1_col3\" class=\"data row1 col3\" >10035.692158</td>\n",
       "      <td id=\"T_0f3d6_row1_col4\" class=\"data row1 col4\" >0.0000</td>\n",
       "      <td id=\"T_0f3d6_row1_col5\" class=\"data row1 col5\" >Yes</td>\n",
       "      <td id=\"T_0f3d6_row1_col6\" class=\"data row1 col6\" >0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_0f3d6_row2_col0\" class=\"data row2 col0\" >SEC_ltm250</td>\n",
       "      <td id=\"T_0f3d6_row2_col1\" class=\"data row2 col1\" >10351.387695</td>\n",
       "      <td id=\"T_0f3d6_row2_col2\" class=\"data row2 col2\" >9451.084386</td>\n",
       "      <td id=\"T_0f3d6_row2_col3\" class=\"data row2 col3\" >11251.691004</td>\n",
       "      <td id=\"T_0f3d6_row2_col4\" class=\"data row2 col4\" >0.0023</td>\n",
       "      <td id=\"T_0f3d6_row2_col5\" class=\"data row2 col5\" >Yes</td>\n",
       "      <td id=\"T_0f3d6_row2_col6\" class=\"data row2 col6\" >0.009365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_0f3d6_row3_col0\" class=\"data row3 col0\" >SEC_ltm500</td>\n",
       "      <td id=\"T_0f3d6_row3_col1\" class=\"data row3 col1\" >12031.593750</td>\n",
       "      <td id=\"T_0f3d6_row3_col2\" class=\"data row3 col2\" >11460.369018</td>\n",
       "      <td id=\"T_0f3d6_row3_col3\" class=\"data row3 col3\" >12602.818482</td>\n",
       "      <td id=\"T_0f3d6_row3_col4\" class=\"data row3 col4\" >0.7972</td>\n",
       "      <td id=\"T_0f3d6_row3_col5\" class=\"data row3 col5\" >No</td>\n",
       "      <td id=\"T_0f3d6_row3_col6\" class=\"data row3 col6\" >1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1b6d1377dc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage:\n",
    "task = 'Memory Constraints SEC'\n",
    "\n",
    "# Define file paths\n",
    "file_path = '../data/exp2_memory_constraints/'\n",
    "plot_path = '../data/exp2_memory_constraints/'\n",
    "\n",
    "# Load data\n",
    "data = load_plot_data('data_true', file_path)  # Ensure this function returns the correct structure\n",
    "data.keys()\n",
    "\n",
    "# Define models and labels\n",
    "models_SEC = ['SEC_ltm125', 'SEC_ltm250', 'SEC_ltm500', 'SEC_ltm1000']\n",
    "#models_NSEC = ['NSEC_ltm125', 'NSEC_ltm250', 'NSEC_ltm500', 'NSEC_ltm1000']\n",
    "    \n",
    "main_statistical_analysis(data, models_SEC, base_model='SEC_ltm1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_truerward_sum  148484.38\n",
      "total reward mean  7424.2197\n",
      "total reward std  3074.0898\n",
      "total reward sem 687.3873859566706\n",
      "model_truerward_sum  80159.71\n",
      "total comp reward mean  3817.1301\n",
      "total comp reward std  2194.6387\n",
      "total comp reward sem 478.90942080688285\n",
      "model_truerward_sum  103730.336\n",
      "total comp reward mean  5186.517\n",
      "total comp reward std  2860.7925\n",
      "total comp reward sem 639.6926455848364\n",
      "model_truerward_sum  123633.234\n",
      "total comp reward mean  6181.6616\n",
      "total comp reward std  2764.6687\n",
      "total comp reward sem 618.1987151086365\n",
      "\n",
      "**Statistical Test Results (Base Model: SEC)**\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zerat\\AppData\\Local\\Temp\\ipykernel_18988\\2656814379.py:142: FutureWarning: this method is deprecated in favour of `Styler.hide(axis='index')`\n",
      "  display(results_df.style.hide_index().set_table_styles(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_de4fc th {\n",
       "  font-size: 12pt;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_de4fc_row0_col0, #T_de4fc_row0_col1, #T_de4fc_row0_col2, #T_de4fc_row0_col3, #T_de4fc_row0_col4, #T_de4fc_row0_col5, #T_de4fc_row0_col6, #T_de4fc_row1_col0, #T_de4fc_row1_col1, #T_de4fc_row1_col2, #T_de4fc_row1_col3, #T_de4fc_row1_col4, #T_de4fc_row1_col5, #T_de4fc_row1_col6, #T_de4fc_row2_col0, #T_de4fc_row2_col1, #T_de4fc_row2_col2, #T_de4fc_row2_col3, #T_de4fc_row2_col4, #T_de4fc_row2_col5, #T_de4fc_row2_col6, #T_de4fc_row3_col0, #T_de4fc_row3_col1, #T_de4fc_row3_col2, #T_de4fc_row3_col3, #T_de4fc_row3_col4, #T_de4fc_row3_col5, #T_de4fc_row3_col6 {\n",
       "  font-size: 12pt;\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_de4fc\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_de4fc_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_de4fc_level0_col1\" class=\"col_heading level0 col1\" >Mean</th>\n",
       "      <th id=\"T_de4fc_level0_col2\" class=\"col_heading level0 col2\" >95% CI Lower</th>\n",
       "      <th id=\"T_de4fc_level0_col3\" class=\"col_heading level0 col3\" >95% CI Upper</th>\n",
       "      <th id=\"T_de4fc_level0_col4\" class=\"col_heading level0 col4\" >p-value</th>\n",
       "      <th id=\"T_de4fc_level0_col5\" class=\"col_heading level0 col5\" >Significant</th>\n",
       "      <th id=\"T_de4fc_level0_col6\" class=\"col_heading level0 col6\" >Adjusted p-value (Bonferroni)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_de4fc_row0_col0\" class=\"data row0 col0\" >NSEC_ltm1000</td>\n",
       "      <td id=\"T_de4fc_row0_col1\" class=\"data row0 col1\" >7424.219727</td>\n",
       "      <td id=\"T_de4fc_row0_col2\" class=\"data row0 col2\" >6076.940450</td>\n",
       "      <td id=\"T_de4fc_row0_col3\" class=\"data row0 col3\" >8771.499003</td>\n",
       "      <td id=\"T_de4fc_row0_col4\" class=\"data row0 col4\" >N/A</td>\n",
       "      <td id=\"T_de4fc_row0_col5\" class=\"data row0 col5\" >None</td>\n",
       "      <td id=\"T_de4fc_row0_col6\" class=\"data row0 col6\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_de4fc_row1_col0\" class=\"data row1 col0\" >NSEC_ltm125</td>\n",
       "      <td id=\"T_de4fc_row1_col1\" class=\"data row1 col1\" >3817.130127</td>\n",
       "      <td id=\"T_de4fc_row1_col2\" class=\"data row1 col2\" >2878.467662</td>\n",
       "      <td id=\"T_de4fc_row1_col3\" class=\"data row1 col3\" >4755.792592</td>\n",
       "      <td id=\"T_de4fc_row1_col4\" class=\"data row1 col4\" >0.0007</td>\n",
       "      <td id=\"T_de4fc_row1_col5\" class=\"data row1 col5\" >Yes</td>\n",
       "      <td id=\"T_de4fc_row1_col6\" class=\"data row1 col6\" >0.002659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_de4fc_row2_col0\" class=\"data row2 col0\" >NSEC_ltm250</td>\n",
       "      <td id=\"T_de4fc_row2_col1\" class=\"data row2 col1\" >5186.517090</td>\n",
       "      <td id=\"T_de4fc_row2_col2\" class=\"data row2 col2\" >3932.719504</td>\n",
       "      <td id=\"T_de4fc_row2_col3\" class=\"data row2 col3\" >6440.314675</td>\n",
       "      <td id=\"T_de4fc_row2_col4\" class=\"data row2 col4\" >0.0411</td>\n",
       "      <td id=\"T_de4fc_row2_col5\" class=\"data row2 col5\" >Yes</td>\n",
       "      <td id=\"T_de4fc_row2_col6\" class=\"data row2 col6\" >0.164494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_de4fc_row3_col0\" class=\"data row3 col0\" >NSEC_ltm500</td>\n",
       "      <td id=\"T_de4fc_row3_col1\" class=\"data row3 col1\" >6181.661621</td>\n",
       "      <td id=\"T_de4fc_row3_col2\" class=\"data row3 col2\" >4969.992139</td>\n",
       "      <td id=\"T_de4fc_row3_col3\" class=\"data row3 col3\" >7393.331103</td>\n",
       "      <td id=\"T_de4fc_row3_col4\" class=\"data row3 col4\" >0.2733</td>\n",
       "      <td id=\"T_de4fc_row3_col5\" class=\"data row3 col5\" >No</td>\n",
       "      <td id=\"T_de4fc_row3_col6\" class=\"data row3 col6\" >1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1b6c1c9c040>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage:\n",
    "task = 'Memory Constraints NSEC'\n",
    "\n",
    "# Define file paths\n",
    "file_path = '../data/exp2_memory_constraints/'\n",
    "plot_path = '../data/exp2_memory_constraints/'\n",
    "\n",
    "# Load data\n",
    "data = load_plot_data('data_true', file_path)  # Ensure this function returns the correct structure\n",
    "data.keys()\n",
    "\n",
    "# Define models and labels\n",
    "#models_SEC = ['SEC_ltm125', 'SEC_ltm250', 'SEC_ltm500', 'SEC_ltm1000']\n",
    "models_NSEC = ['NSEC_ltm125', 'NSEC_ltm250', 'NSEC_ltm500', 'NSEC_ltm1000']\n",
    "    \n",
    "main_statistical_analysis(data, models_NSEC, base_model='NSEC_ltm1000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For exp 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_truerward_sum  165764.45\n",
      "total reward mean  11840.318\n",
      "total reward std  1485.1364\n",
      "total reward sem 396.919385988878\n",
      "model_truerward_sum  140945.31\n",
      "total comp reward mean  10841.947\n",
      "total comp reward std  1985.7986\n",
      "total comp reward sem 550.7614320999575\n",
      "model_truerward_sum  160277.66\n",
      "total comp reward mean  11448.405\n",
      "total comp reward std  1565.0648\n",
      "total comp reward sem 418.2811672891668\n",
      "model_truerward_sum  162697.52\n",
      "total comp reward mean  11621.252\n",
      "total comp reward std  1384.2921\n",
      "total comp reward sem 369.96762962611865\n",
      "\n",
      "**Statistical Test Results (Base Model: SEC)**\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zerat\\AppData\\Local\\Temp\\ipykernel_18988\\2656814379.py:142: FutureWarning: this method is deprecated in favour of `Styler.hide(axis='index')`\n",
      "  display(results_df.style.hide_index().set_table_styles(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_c1857 th {\n",
       "  font-size: 12pt;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_c1857_row0_col0, #T_c1857_row0_col1, #T_c1857_row0_col2, #T_c1857_row0_col3, #T_c1857_row0_col4, #T_c1857_row0_col5, #T_c1857_row0_col6, #T_c1857_row1_col0, #T_c1857_row1_col1, #T_c1857_row1_col2, #T_c1857_row1_col3, #T_c1857_row1_col4, #T_c1857_row1_col5, #T_c1857_row1_col6, #T_c1857_row2_col0, #T_c1857_row2_col1, #T_c1857_row2_col2, #T_c1857_row2_col3, #T_c1857_row2_col4, #T_c1857_row2_col5, #T_c1857_row2_col6, #T_c1857_row3_col0, #T_c1857_row3_col1, #T_c1857_row3_col2, #T_c1857_row3_col3, #T_c1857_row3_col4, #T_c1857_row3_col5, #T_c1857_row3_col6 {\n",
       "  font-size: 12pt;\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_c1857\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_c1857_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_c1857_level0_col1\" class=\"col_heading level0 col1\" >Mean</th>\n",
       "      <th id=\"T_c1857_level0_col2\" class=\"col_heading level0 col2\" >95% CI Lower</th>\n",
       "      <th id=\"T_c1857_level0_col3\" class=\"col_heading level0 col3\" >95% CI Upper</th>\n",
       "      <th id=\"T_c1857_level0_col4\" class=\"col_heading level0 col4\" >p-value</th>\n",
       "      <th id=\"T_c1857_level0_col5\" class=\"col_heading level0 col5\" >Significant</th>\n",
       "      <th id=\"T_c1857_level0_col6\" class=\"col_heading level0 col6\" >Adjusted p-value (Bonferroni)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_c1857_row0_col0\" class=\"data row0 col0\" >default</td>\n",
       "      <td id=\"T_c1857_row0_col1\" class=\"data row0 col1\" >11840.318359</td>\n",
       "      <td id=\"T_c1857_row0_col2\" class=\"data row0 col2\" >11062.356363</td>\n",
       "      <td id=\"T_c1857_row0_col3\" class=\"data row0 col3\" >12618.280356</td>\n",
       "      <td id=\"T_c1857_row0_col4\" class=\"data row0 col4\" >N/A</td>\n",
       "      <td id=\"T_c1857_row0_col5\" class=\"data row0 col5\" >None</td>\n",
       "      <td id=\"T_c1857_row0_col6\" class=\"data row0 col6\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_c1857_row1_col0\" class=\"data row1 col0\" >noDist</td>\n",
       "      <td id=\"T_c1857_row1_col1\" class=\"data row1 col1\" >10841.947266</td>\n",
       "      <td id=\"T_c1857_row1_col2\" class=\"data row1 col2\" >9762.454859</td>\n",
       "      <td id=\"T_c1857_row1_col3\" class=\"data row1 col3\" >11921.439673</td>\n",
       "      <td id=\"T_c1857_row1_col4\" class=\"data row1 col4\" >0.1523</td>\n",
       "      <td id=\"T_c1857_row1_col5\" class=\"data row1 col5\" >No</td>\n",
       "      <td id=\"T_c1857_row1_col6\" class=\"data row1 col6\" >0.609123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_c1857_row2_col0\" class=\"data row2 col0\" >noRR</td>\n",
       "      <td id=\"T_c1857_row2_col1\" class=\"data row2 col1\" >11448.405273</td>\n",
       "      <td id=\"T_c1857_row2_col2\" class=\"data row2 col2\" >10628.574186</td>\n",
       "      <td id=\"T_c1857_row2_col3\" class=\"data row2 col3\" >12268.236361</td>\n",
       "      <td id=\"T_c1857_row2_col4\" class=\"data row2 col4\" >0.4213</td>\n",
       "      <td id=\"T_c1857_row2_col5\" class=\"data row2 col5\" >No</td>\n",
       "      <td id=\"T_c1857_row2_col6\" class=\"data row2 col6\" >1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_c1857_row3_col0\" class=\"data row3 col0\" >noGi_new</td>\n",
       "      <td id=\"T_c1857_row3_col1\" class=\"data row3 col1\" >11621.251953</td>\n",
       "      <td id=\"T_c1857_row3_col2\" class=\"data row3 col2\" >10896.115399</td>\n",
       "      <td id=\"T_c1857_row3_col3\" class=\"data row3 col3\" >12346.388507</td>\n",
       "      <td id=\"T_c1857_row3_col4\" class=\"data row3 col4\" >0.5053</td>\n",
       "      <td id=\"T_c1857_row3_col5\" class=\"data row3 col5\" >No</td>\n",
       "      <td id=\"T_c1857_row3_col6\" class=\"data row3 col6\" >1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1b6b56f6d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage:\n",
    "task = '1ablation'\n",
    "\n",
    "# Define file paths\n",
    "file_path = '../data/exp3_ablations/'\n",
    "plot_path = '../data/exp3_ablations/'\n",
    "\n",
    "# Load data\n",
    "data = load_plot_data('data_new', file_path)  # Ensure this function returns the correct structure\n",
    "data.keys()\n",
    "\n",
    "# Define models and labels\n",
    "#models_tmaze = ['SEC', 'NSEC', 'MFEC-ae', 'MFEC-rp', 'DQN', 'ERLAM']\n",
    "#models_cylinder = ['SEC_500', 'NSEC_500', 'MFEC_VAE_100K', 'MFEC_RP_100K', 'DQN_25k', 'ERLAM_5k']\n",
    "#models_detour = ['SEC_500', 'NSEC_500', 'MFEC_VAE_100K', 'MFEC_RP_100K', 'DQN_25k', 'ERLAM_25k']\n",
    "#models_permanence = ['SEC_500', 'NSEC_500', 'MFEC_VAE_100K', 'MFEC_RP_100K', 'DQN_25k', 'ERLAM_5k']\n",
    "\n",
    "models_1ablation = ['noDist', 'noRR', 'noGi_new', 'default']\n",
    "models_2ablation = ['soloDist', 'soloRR', 'soloGi', 'default']\n",
    "\n",
    "#labels = ['SEC', 'NSEC', 'MFEC-ae', 'MFEC-rp', 'DQN', 'ERLAM']\n",
    "    \n",
    "main_statistical_analysis(data, models_1ablation, base_model='default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For exp 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_truerward_sum  100490.64\n",
      "total reward mean  5024.5312\n",
      "total reward std  2889.6023\n",
      "total reward sem 646.1347159384708\n",
      "model_truerward_sum  103469.84\n",
      "total comp reward mean  5173.492\n",
      "total comp reward std  3293.304\n",
      "total comp reward sem 736.40515141236\n",
      "model_truerward_sum  133381.92\n",
      "total comp reward mean  6669.096\n",
      "total comp reward std  2805.5818\n",
      "total comp reward sem 627.3471592411905\n",
      "\n",
      "**Statistical Test Results (Base Model: SEC)**\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zerat\\AppData\\Local\\Temp\\ipykernel_9924\\2656814379.py:142: FutureWarning: this method is deprecated in favour of `Styler.hide(axis='index')`\n",
      "  display(results_df.style.hide_index().set_table_styles(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_6fb20 th {\n",
       "  font-size: 12pt;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_6fb20_row0_col0, #T_6fb20_row0_col1, #T_6fb20_row0_col2, #T_6fb20_row0_col3, #T_6fb20_row0_col4, #T_6fb20_row0_col5, #T_6fb20_row0_col6, #T_6fb20_row1_col0, #T_6fb20_row1_col1, #T_6fb20_row1_col2, #T_6fb20_row1_col3, #T_6fb20_row1_col4, #T_6fb20_row1_col5, #T_6fb20_row1_col6, #T_6fb20_row2_col0, #T_6fb20_row2_col1, #T_6fb20_row2_col2, #T_6fb20_row2_col3, #T_6fb20_row2_col4, #T_6fb20_row2_col5, #T_6fb20_row2_col6 {\n",
       "  font-size: 12pt;\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_6fb20\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_6fb20_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_6fb20_level0_col1\" class=\"col_heading level0 col1\" >Mean</th>\n",
       "      <th id=\"T_6fb20_level0_col2\" class=\"col_heading level0 col2\" >95% CI Lower</th>\n",
       "      <th id=\"T_6fb20_level0_col3\" class=\"col_heading level0 col3\" >95% CI Upper</th>\n",
       "      <th id=\"T_6fb20_level0_col4\" class=\"col_heading level0 col4\" >p-value</th>\n",
       "      <th id=\"T_6fb20_level0_col5\" class=\"col_heading level0 col5\" >Significant</th>\n",
       "      <th id=\"T_6fb20_level0_col6\" class=\"col_heading level0 col6\" >Adjusted p-value (Bonferroni)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_6fb20_row0_col0\" class=\"data row0 col0\" >NSEC</td>\n",
       "      <td id=\"T_6fb20_row0_col1\" class=\"data row0 col1\" >5024.531250</td>\n",
       "      <td id=\"T_6fb20_row0_col2\" class=\"data row0 col2\" >3758.107207</td>\n",
       "      <td id=\"T_6fb20_row0_col3\" class=\"data row0 col3\" >6290.955293</td>\n",
       "      <td id=\"T_6fb20_row0_col4\" class=\"data row0 col4\" >N/A</td>\n",
       "      <td id=\"T_6fb20_row0_col5\" class=\"data row0 col5\" >None</td>\n",
       "      <td id=\"T_6fb20_row0_col6\" class=\"data row0 col6\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6fb20_row1_col0\" class=\"data row1 col0\" >NSEC-FIFO</td>\n",
       "      <td id=\"T_6fb20_row1_col1\" class=\"data row1 col1\" >5173.492188</td>\n",
       "      <td id=\"T_6fb20_row1_col2\" class=\"data row1 col2\" >3730.138091</td>\n",
       "      <td id=\"T_6fb20_row1_col3\" class=\"data row1 col3\" >6616.846284</td>\n",
       "      <td id=\"T_6fb20_row1_col4\" class=\"data row1 col4\" >0.9246</td>\n",
       "      <td id=\"T_6fb20_row1_col5\" class=\"data row1 col5\" >No</td>\n",
       "      <td id=\"T_6fb20_row1_col6\" class=\"data row1 col6\" >1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6fb20_row2_col0\" class=\"data row2 col0\" >NSEC-RWD</td>\n",
       "      <td id=\"T_6fb20_row2_col1\" class=\"data row2 col1\" >6669.096191</td>\n",
       "      <td id=\"T_6fb20_row2_col2\" class=\"data row2 col2\" >5439.495759</td>\n",
       "      <td id=\"T_6fb20_row2_col3\" class=\"data row2 col3\" >7898.696624</td>\n",
       "      <td id=\"T_6fb20_row2_col4\" class=\"data row2 col4\" >0.0909</td>\n",
       "      <td id=\"T_6fb20_row2_col5\" class=\"data row2 col5\" >No</td>\n",
       "      <td id=\"T_6fb20_row2_col6\" class=\"data row2 col6\" >0.272722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20f08b3a4f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage:\n",
    "task = 'SECmemory'\n",
    "\n",
    "# Define file paths\n",
    "file_path = '../data/exp4_forgetting/'\n",
    "plot_path = '../data/exp4_forgetting/'\n",
    "\n",
    "# Load data\n",
    "data = load_plot_data('data_new', file_path)  # Ensure this function returns the correct structure\n",
    "data.keys()\n",
    "\n",
    "# Define models and labels\n",
    "#models_tmaze = ['SEC', 'NSEC', 'MFEC-ae', 'MFEC-rp', 'DQN', 'ERLAM']\n",
    "#models_cylinder = ['SEC_500', 'NSEC_500', 'MFEC_VAE_100K', 'MFEC_RP_100K', 'DQN_25k', 'ERLAM_5k']\n",
    "#models_detour = ['SEC_500', 'NSEC_500', 'MFEC_VAE_100K', 'MFEC_RP_100K', 'DQN_25k', 'ERLAM_25k']\n",
    "#models_permanence = ['SEC_500', 'NSEC_500', 'MFEC_VAE_100K', 'MFEC_RP_100K', 'DQN_25k', 'ERLAM_5k']\n",
    "\n",
    "models_sec = ['SEC', 'SEC-PRIOR', 'SEC-FIFO_v2']\n",
    "models_nsec = ['NSEC', 'NSEC-FIFO', 'NSEC-RWD']\n",
    "\n",
    "#labels = ['SEC', 'NSEC', 'MFEC-ae', 'MFEC-rp', 'DQN', 'ERLAM']\n",
    "    \n",
    "main_statistical_analysis(data, models_nsec, base_model='NSEC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated table exp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Task               SEC         NSEC      MFEC-ae      MFEC-rp  \\\n",
      "0      Double T-Maze  **__11604.88__**  **5024.53**  **3525.48**  **1860.34**   \n",
      "1           Cylinder   **__5829.88__**      5130.67      4065.24      5254.58   \n",
      "2             Detour   **__1181.82__**     **3.48**     **3.74**     **4.02**   \n",
      "3  Object Permanence   **__7224.33__**  **3803.15**  **5218.06**  **4904.88**   \n",
      "\n",
      "           DQN        ERLAM  \n",
      "0  **6207.65**  **7826.21**  \n",
      "1  **3490.08**  **4665.28**  \n",
      "2     **8.03**    **24.52**  \n",
      "3  **3850.78**  **3793.70**  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '../data/'\n",
    "\n",
    "# Load the CSV files\n",
    "double_tmaze_df = pd.read_csv(file_path+'DoubleTMaze_statistical_test_results.csv')\n",
    "cylinder_df = pd.read_csv(file_path+'Cylinder_statistical_test_results.csv')\n",
    "detour_df = pd.read_csv(file_path+'Detour_statistical_test_results.csv')\n",
    "permanence_df = pd.read_csv(file_path+'Permanence_statistical_test_results.csv')\n",
    "\n",
    "# Define the function to generate the comparison table\n",
    "def generate_comparison_table_v3(task_dfs, task_names, models, base_model='SEC'):\n",
    "    comparison_data = []\n",
    "    \n",
    "    # Iterate through each task's dataframe\n",
    "    for task_df, task_name in zip(task_dfs, task_names):\n",
    "        task_data = {'Task': task_name}\n",
    "        \n",
    "        # Add the mean performance for each model and format accordingly\n",
    "        base_mean = task_df[task_df['Model'] == base_model]['Mean'].values[0]\n",
    "        best_model = None\n",
    "        best_mean = -float('inf')\n",
    "\n",
    "        # Find the best performing model\n",
    "        for model in models:\n",
    "            model_mean = task_df[task_df['Model'] == model]['Mean'].values[0]\n",
    "            if model_mean > best_mean:\n",
    "                best_mean = model_mean\n",
    "                best_model = model\n",
    "\n",
    "        # Populate comparison data, with formatting\n",
    "        for model in models:\n",
    "            model_mean = task_df[task_df['Model'] == model]['Mean'].values[0]\n",
    "            is_significant = task_df[task_df['Model'] == model]['Significant'].values[0] == 'No'\n",
    "\n",
    "            if model == best_model:\n",
    "                # Highlight best performing model (bold + underline)\n",
    "                task_data[model] = f\"**__{model_mean:.2f}__**\"\n",
    "            elif not is_significant:\n",
    "                # Highlight models not statistically significant from SEC (bold)\n",
    "                task_data[model] = f\"**{model_mean:.2f}**\"\n",
    "            else:\n",
    "                # Normal formatting for significant models\n",
    "                task_data[model] = f\"{model_mean:.2f}\"\n",
    "        \n",
    "        comparison_data.append(task_data)\n",
    "    \n",
    "    # Create DataFrame from comparison data\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    return comparison_df\n",
    "\n",
    "# Define task names and models\n",
    "task_names = ['Double T-Maze', 'Cylinder', 'Detour', 'Object Permanence']\n",
    "models = ['SEC', 'NSEC', 'MFEC-ae', 'MFEC-rp', 'DQN', 'ERLAM']\n",
    "\n",
    "# Generate the comparison table\n",
    "comparison_df_v3 = generate_comparison_table_v3(\n",
    "    task_dfs=[double_tmaze_df, cylinder_df, detour_df, permanence_df],\n",
    "    task_names=task_names,\n",
    "    models=models\n",
    ")\n",
    "\n",
    "# Display the table (replace with your display mechanism)\n",
    "print(comparison_df_v3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_performance_comparison(results_df):\n",
    "    \"\"\"\n",
    "    Display the performance comparison results in a formatted table.\n",
    "    \n",
    "    Parameters:\n",
    "    - results_df: Pandas DataFrame containing the model performance comparison.\n",
    "    \"\"\"\n",
    "    # Display the table with formatting similar to your previous example\n",
    "    print(\"\\n**Model Performance Comparison**\\n\")\n",
    "    \n",
    "    # Format and display the DataFrame with styled output\n",
    "    display(results_df.style.hide_index().set_table_styles(\n",
    "        [   \n",
    "            # Style for the header\n",
    "            {'selector': 'th', 'props': [('font-size', '12pt'), ('text-align', 'center')]},\n",
    "            # Style for the body\n",
    "            {'selector': 'td', 'props': [('font-size', '12pt'), ('text-align', 'center')]}\n",
    "        ]\n",
    "    ).set_properties(**{\n",
    "        'text-align': 'center'\n",
    "    }))\n",
    "    \n",
    "def display_and_save_dataframe_v3(df, filename=\"comparison_results.csv\"):\n",
    "    \"\"\"\n",
    "    Displays the DataFrame using a custom formatting function and saves it to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The DataFrame to display and save.\n",
    "    - filename: The name of the CSV file to save the DataFrame (default is 'comparison_results.csv').\n",
    "    \"\"\"\n",
    "    # Display the DataFrame using the custom display function\n",
    "    display_performance_comparison(df)\n",
    "    \n",
    "    # Save the dataframe to a CSV file\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "# Call the function to display and save the dataframe\n",
    "display_and_save_dataframe_v3(comparison_df_v3, filename=\"consistent_model_performance_comparison.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated table exp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the function to handle different model sets for each task separately\n",
    "def generate_comparison_table_v5(task_dfs, task_names, model_lists, base_model='SEC'):\n",
    "    comparison_data = []\n",
    "    \n",
    "    # Iterate through each task's dataframe and corresponding model list\n",
    "    for task_df, task_name, models in zip(task_dfs, task_names, model_lists):\n",
    "        task_data = {'Task': task_name}\n",
    "        \n",
    "        # Add the mean performance for each model and format accordingly\n",
    "        base_mean = task_df[task_df['Model'] == base_model]['Mean'].values[0]\n",
    "        best_model = None\n",
    "        best_mean = -float('inf')\n",
    "\n",
    "        # Find the best performing model\n",
    "        for model in models:\n",
    "            model_mean = task_df[task_df['Model'] == model]['Mean'].values[0]\n",
    "            if model_mean > best_mean:\n",
    "                best_mean = model_mean\n",
    "                best_model = model\n",
    "\n",
    "        # Populate comparison data, with formatting\n",
    "        for model in models:\n",
    "            model_mean = task_df[task_df['Model'] == model]['Mean'].values[0]\n",
    "            is_significant = task_df[task_df['Model'] == model]['Significant'].values[0] == 'No'\n",
    "\n",
    "            if model == best_model:\n",
    "                # Highlight best performing model (bold + underline)\n",
    "                task_data[model] = f\"**__{model_mean:.2f}__**\"\n",
    "            elif not is_significant:\n",
    "                # Highlight models not statistically significant from SEC (bold)\n",
    "                task_data[model] = f\"**{model_mean:.2f}**\"\n",
    "            else:\n",
    "                # Normal formatting for significant models\n",
    "                task_data[model] = f\"{model_mean:.2f}\"\n",
    "        \n",
    "        comparison_data.append(task_data)\n",
    "    \n",
    "    # Create DataFrame from comparison data\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    return comparison_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/'\n",
    "\n",
    "# Load the CSV files\n",
    "ablation_1_df = pd.read_csv(file_path+'1ablation_statistical_test_results.csv')\n",
    "ablation_2_df = pd.read_csv(file_path+'2ablation_statistical_test_results.csv')\n",
    "task_names = ['Ablation Study 1', 'Ablation Study 2']\n",
    "models_1ablation = ['SEC', 'noDist', 'noRR', 'noGi']\n",
    "models_2ablation = ['SEC', 'soloDist', 'soloRR', 'soloGi']\n",
    "\n",
    "#print(ablation_1_df['Model'].unique())\n",
    "#print(ablation_2_df['Model'].unique())\n",
    "\n",
    "# Generate the comparison table with the updated function\n",
    "comparison_df_v5 = generate_comparison_table_v5(\n",
    "    task_dfs=[ablation_1_df, ablation_2_df],\n",
    "    task_names=task_names,\n",
    "    model_lists=[models_1ablation, models_2ablation]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comparison_df_v5' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 40>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_csv(filename, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Call the function to display and save the dataframe\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m display_and_save_dataframe_v3(\u001b[43mcomparison_df_v5\u001b[49m, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mablation_studies_performance_comparison.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'comparison_df_v5' is not defined"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "def display_performance_comparison(results_df):\n",
    "    \"\"\"\n",
    "    Display the performance comparison results in a formatted table.\n",
    "    \n",
    "    Parameters:\n",
    "    - results_df: Pandas DataFrame containing the model performance comparison.\n",
    "    \"\"\"\n",
    "    # Display the table with formatting\n",
    "    print(\"\\n**Model Performance Comparison**\\n\")\n",
    "    \n",
    "    # Format and display the DataFrame with styled output\n",
    "    display(results_df.style.hide_index().set_table_styles(\n",
    "        [   \n",
    "            # Style for the header\n",
    "            {'selector': 'th', 'props': [('font-size', '12pt'), ('text-align', 'center')]},\n",
    "            # Style for the body\n",
    "            {'selector': 'td', 'props': [('font-size', '12pt'), ('text-align', 'center')]}\n",
    "        ]\n",
    "    ).set_properties(**{\n",
    "        'text-align': 'center'\n",
    "    }))\n",
    "    \n",
    "def display_and_save_dataframe_v3(df, filename=\"comparison_results.csv\"):\n",
    "    \"\"\"\n",
    "    Displays the DataFrame using a custom formatting function and saves it to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The DataFrame to display and save.\n",
    "    - filename: The name of the CSV file to save the DataFrame (default is 'comparison_results.csv').\n",
    "    \"\"\"\n",
    "    # Display the DataFrame using the custom display function\n",
    "    display_performance_comparison(df)\n",
    "    \n",
    "    # Save the dataframe to a CSV file\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "# Call the function to display and save the dataframe\n",
    "display_and_save_dataframe_v3(comparison_df_v5, filename=\"ablation_studies_performance_comparison.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated table exp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the generate_comparison_table_v5 function to accept different base models for each table\n",
    "def generate_two_comparison_tables(task_dfs, task_names, model_lists, base_models):\n",
    "    \"\"\"\n",
    "    Generate two comparison tables with different base models for each dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - task_dfs: List of DataFrames corresponding to the tasks.\n",
    "    - task_names: List of task names.\n",
    "    - model_lists: List of model names for each task.\n",
    "    - base_models: List of base models to use for each dataset.\n",
    "\n",
    "    Returns:\n",
    "    - List of two DataFrames: one for each base model.\n",
    "    \"\"\"\n",
    "    comparison_data_sec = []\n",
    "    comparison_data_nsec = []\n",
    "\n",
    "    for task_df, task_name, models, base_model in zip(task_dfs, task_names, model_lists, base_models):\n",
    "        task_data = {'Task': task_name}\n",
    "        \n",
    "        # Check if the base model exists in the dataframe\n",
    "        if base_model in task_df['Model'].values:\n",
    "            base_mean = task_df[task_df['Model'] == base_model]['Mean'].values[0]\n",
    "        else:\n",
    "            print(f\"Base model {base_model} not found in {task_name}\")\n",
    "            continue  # Skip this task if base model not found\n",
    "\n",
    "        best_model = None\n",
    "        best_mean = -float('inf')\n",
    "\n",
    "        # Find the best performing model\n",
    "        for model in models:\n",
    "            if model in task_df['Model'].values:\n",
    "                model_mean = task_df[task_df['Model'] == model]['Mean'].values[0]\n",
    "                if model_mean > best_mean:\n",
    "                    best_mean = model_mean\n",
    "                    best_model = model\n",
    "            else:\n",
    "                print(f\"Model {model} not found in {task_name}\")\n",
    "\n",
    "        # Populate comparison data, with formatting\n",
    "        for model in models:\n",
    "            if model in task_df['Model'].values:\n",
    "                model_mean = task_df[task_df['Model'] == model]['Mean'].values[0]\n",
    "                is_significant = task_df[task_df['Model'] == model]['Significant'].values[0] == 'No'\n",
    "\n",
    "                if model == best_model:\n",
    "                    # Highlight best performing model (bold + underline)\n",
    "                    task_data[model] = f\"**__{model_mean:.2f}__**\"\n",
    "                elif not is_significant:\n",
    "                    # Highlight models not statistically significant from SEC (bold)\n",
    "                    task_data[model] = f\"**{model_mean:.2f}**\"\n",
    "                else:\n",
    "                    # Normal formatting for significant models\n",
    "                    task_data[model] = f\"{model_mean:.2f}\"\n",
    "            else:\n",
    "                task_data[model] = 'N/A'  # If model not found, mark as N/A\n",
    "        \n",
    "        # Append to the appropriate comparison list\n",
    "        if base_model == \"SEC\":\n",
    "            comparison_data_sec.append(task_data)\n",
    "        else:\n",
    "            comparison_data_nsec.append(task_data)\n",
    "    \n",
    "    # Create DataFrame from comparison data for each base model\n",
    "    comparison_df_sec = pd.DataFrame(comparison_data_sec)\n",
    "    comparison_df_nsec = pd.DataFrame(comparison_data_nsec)\n",
    "    \n",
    "    return comparison_df_sec, comparison_df_nsec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/'\n",
    "\n",
    "# Load the CSV files\n",
    "memory_sec_df = pd.read_csv(file_path+'SECforgetting_statistical_test_results_new.csv')\n",
    "memory_nsec_df = pd.read_csv(file_path+'NSECforgetting_statistical_test_results_new.csv')\n",
    "# Use the function for SEC and NSEC as base models\n",
    "task_dfs = [memory_sec_df, memory_nsec_df]\n",
    "task_names = ['SEC forgetting', 'NSEC forgetting']\n",
    "model_lists = [['SEC', 'SEC-fifo', 'SEC-rwd'], ['NSEC', 'NSEC-fifo', 'NSEC-rwd']]\n",
    "base_models = ['SEC', 'NSEC']\n",
    "\n",
    "\n",
    "#print(memory_sec_df['Model'].unique())\n",
    "#print(memory_nsec_df['Model'].unique())\n",
    "\n",
    "\n",
    "# Generate the two tables\n",
    "comparison_df_sec, comparison_df_nsec = generate_two_comparison_tables(\n",
    "    task_dfs=task_dfs,\n",
    "    task_names=task_names,\n",
    "    model_lists=model_lists,\n",
    "    base_models=base_models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Model Performance Comparison**\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zerat\\AppData\\Local\\Temp\\ipykernel_9924\\3706623491.py:12: FutureWarning: this method is deprecated in favour of `Styler.hide(axis='index')`\n",
      "  display(results_df.style.hide_index().set_table_styles(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_71c09 th {\n",
       "  font-size: 12pt;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_71c09 td {\n",
       "  font-size: 12pt;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_71c09_row0_col0, #T_71c09_row0_col1, #T_71c09_row0_col2, #T_71c09_row0_col3 {\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_71c09\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_71c09_level0_col0\" class=\"col_heading level0 col0\" >Task</th>\n",
       "      <th id=\"T_71c09_level0_col1\" class=\"col_heading level0 col1\" >SEC</th>\n",
       "      <th id=\"T_71c09_level0_col2\" class=\"col_heading level0 col2\" >SEC-fifo</th>\n",
       "      <th id=\"T_71c09_level0_col3\" class=\"col_heading level0 col3\" >SEC-rwd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_71c09_row0_col0\" class=\"data row0 col0\" >SEC forgetting</td>\n",
       "      <td id=\"T_71c09_row0_col1\" class=\"data row0 col1\" >**11604.88**</td>\n",
       "      <td id=\"T_71c09_row0_col2\" class=\"data row0 col2\" >**__12401.42__**</td>\n",
       "      <td id=\"T_71c09_row0_col3\" class=\"data row0 col3\" >12191.91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20f4457a9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the function to display and save the dataframe\n",
    "display_and_save_dataframe_v3(comparison_df_sec, filename=\"SECforgetting_studies_performance_comparison_new.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Model Performance Comparison**\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zerat\\AppData\\Local\\Temp\\ipykernel_9924\\3706623491.py:12: FutureWarning: this method is deprecated in favour of `Styler.hide(axis='index')`\n",
      "  display(results_df.style.hide_index().set_table_styles(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7bb4a th {\n",
       "  font-size: 12pt;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_7bb4a td {\n",
       "  font-size: 12pt;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_7bb4a_row0_col0, #T_7bb4a_row0_col1, #T_7bb4a_row0_col2, #T_7bb4a_row0_col3 {\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7bb4a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_7bb4a_level0_col0\" class=\"col_heading level0 col0\" >Task</th>\n",
       "      <th id=\"T_7bb4a_level0_col1\" class=\"col_heading level0 col1\" >NSEC</th>\n",
       "      <th id=\"T_7bb4a_level0_col2\" class=\"col_heading level0 col2\" >NSEC-fifo</th>\n",
       "      <th id=\"T_7bb4a_level0_col3\" class=\"col_heading level0 col3\" >NSEC-rwd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_7bb4a_row0_col0\" class=\"data row0 col0\" >NSEC forgetting</td>\n",
       "      <td id=\"T_7bb4a_row0_col1\" class=\"data row0 col1\" >**5024.53**</td>\n",
       "      <td id=\"T_7bb4a_row0_col2\" class=\"data row0 col2\" >5173.49</td>\n",
       "      <td id=\"T_7bb4a_row0_col3\" class=\"data row0 col3\" >**__6669.10__**</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20f4462e1f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_and_save_dataframe_v3(comparison_df_nsec, filename=\"NSECforgetting_studies_performance_comparison_new.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated table exp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SEC_ltm1000' 'SEC_ltm125' 'SEC_ltm250' 'SEC_ltm500']\n",
      "['NSEC_ltm1000' 'NSEC_ltm125' 'NSEC_ltm250' 'NSEC_ltm500']\n",
      "Model SEC_ltm1000SEC_ltm125SEC_ltm250SEC_ltm500 not found in SEC memory constraints\n",
      "Model NSEC_ltm1000NSEC_ltm125NSEC_ltm250NSEC_ltm500 not found in NSEC memory constraints\n"
     ]
    }
   ],
   "source": [
    "file_path = '../data/'\n",
    "\n",
    "# Load the CSV files\n",
    "memory_sec_df = pd.read_csv(file_path+'Memory Constraints SEC_statistical_test_results.csv')\n",
    "memory_nsec_df = pd.read_csv(file_path+'Memory Constraints NSEC_statistical_test_results.csv')\n",
    "# Use the function for SEC and NSEC as base models\n",
    "task_dfs = [memory_sec_df, memory_nsec_df]\n",
    "task_names = ['SEC memory constraints', 'NSEC memory constraints']\n",
    "model_lists = [['SEC_ltm1000' 'SEC_ltm125' 'SEC_ltm250' 'SEC_ltm500'], ['NSEC_ltm1000' 'NSEC_ltm125' 'NSEC_ltm250' 'NSEC_ltm500']]\n",
    "base_models = ['SEC_ltm1000', 'NSEC_ltm1000']\n",
    "\n",
    "print(memory_sec_df['Model'].unique())\n",
    "print(memory_nsec_df['Model'].unique())\n",
    "\n",
    "# Generate the two tables\n",
    "comparison_df_sec, comparison_df_nsec = generate_two_comparison_tables(\n",
    "    task_dfs=task_dfs,\n",
    "    task_names=task_names,\n",
    "    model_lists=model_lists,\n",
    "    base_models=base_models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Model Performance Comparison**\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zerat\\AppData\\Local\\Temp\\ipykernel_18988\\1298627467.py:14: FutureWarning: this method is deprecated in favour of `Styler.hide(axis='index')`\n",
      "  display(results_df.style.hide_index().set_table_styles(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_2f022 th {\n",
       "  font-size: 12pt;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_2f022 td {\n",
       "  font-size: 12pt;\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_2f022\">\n",
       "  <thead>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1b69d732910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the function to display and save the dataframe\n",
    "display_and_save_dataframe_v3(comparison_df_sec, filename=\"SECmemory_studies_performance_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to display and save the dataframe\n",
    "display_and_save_dataframe_v3(comparison_df_sec, filename=\"NSECmemory_studies_performance_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**SEC Memory Constraints Comparison**\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zerat\\AppData\\Local\\Temp\\ipykernel_18988\\1377185438.py:83: FutureWarning: this method is deprecated in favour of `Styler.hide(axis='index')`\n",
      "  display(df.style.set_caption(table_name).hide_index().set_table_styles(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_5dd84 th {\n",
       "  font-size: 12pt;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_5dd84 td {\n",
       "  font-size: 12pt;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_5dd84_row0_col0, #T_5dd84_row0_col1, #T_5dd84_row0_col2, #T_5dd84_row0_col3, #T_5dd84_row0_col4 {\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_5dd84\">\n",
       "  <caption>SEC Memory Constraints Comparison</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_5dd84_level0_col0\" class=\"col_heading level0 col0\" >Task</th>\n",
       "      <th id=\"T_5dd84_level0_col1\" class=\"col_heading level0 col1\" >SEC_ltm1000</th>\n",
       "      <th id=\"T_5dd84_level0_col2\" class=\"col_heading level0 col2\" >SEC_ltm125</th>\n",
       "      <th id=\"T_5dd84_level0_col3\" class=\"col_heading level0 col3\" >SEC_ltm250</th>\n",
       "      <th id=\"T_5dd84_level0_col4\" class=\"col_heading level0 col4\" >SEC_ltm500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_5dd84_row0_col0\" class=\"data row0 col0\" >SEC memory constraints</td>\n",
       "      <td id=\"T_5dd84_row0_col1\" class=\"data row0 col1\" >**__12174.31__**</td>\n",
       "      <td id=\"T_5dd84_row0_col2\" class=\"data row0 col2\" >**9241.33**</td>\n",
       "      <td id=\"T_5dd84_row0_col3\" class=\"data row0 col3\" >**10351.39**</td>\n",
       "      <td id=\"T_5dd84_row0_col4\" class=\"data row0 col4\" >12031.59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1b6993c96a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table saved as SEC_memory_constraints_comparison.csv\n",
      "\n",
      "**NSEC Memory Constraints Comparison**\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_2e12b th {\n",
       "  font-size: 12pt;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_2e12b td {\n",
       "  font-size: 12pt;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_2e12b_row0_col0, #T_2e12b_row0_col1, #T_2e12b_row0_col2, #T_2e12b_row0_col3, #T_2e12b_row0_col4 {\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_2e12b\">\n",
       "  <caption>NSEC Memory Constraints Comparison</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_2e12b_level0_col0\" class=\"col_heading level0 col0\" >Task</th>\n",
       "      <th id=\"T_2e12b_level0_col1\" class=\"col_heading level0 col1\" >NSEC_ltm1000</th>\n",
       "      <th id=\"T_2e12b_level0_col2\" class=\"col_heading level0 col2\" >NSEC_ltm125</th>\n",
       "      <th id=\"T_2e12b_level0_col3\" class=\"col_heading level0 col3\" >NSEC_ltm250</th>\n",
       "      <th id=\"T_2e12b_level0_col4\" class=\"col_heading level0 col4\" >NSEC_ltm500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_2e12b_row0_col0\" class=\"data row0 col0\" >NSEC memory constraints</td>\n",
       "      <td id=\"T_2e12b_row0_col1\" class=\"data row0 col1\" >**__7424.22__**</td>\n",
       "      <td id=\"T_2e12b_row0_col2\" class=\"data row0 col2\" >**3817.13**</td>\n",
       "      <td id=\"T_2e12b_row0_col3\" class=\"data row0 col3\" >**5186.52**</td>\n",
       "      <td id=\"T_2e12b_row0_col4\" class=\"data row0 col4\" >6181.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1b69d2f42e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table saved as NSEC_memory_constraints_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the uploaded CSV files for memory constraints analysis\n",
    "memory_sec_df = pd.read_csv(file_path+'Memory Constraints SEC_statistical_test_results.csv')\n",
    "memory_nsec_df = pd.read_csv(file_path+'Memory Constraints NSEC_statistical_test_results.csv')\n",
    "\n",
    "# Define task names, model lists, and base models for SEC and NSEC\n",
    "task_dfs = [memory_sec_df, memory_nsec_df]\n",
    "task_names = ['SEC memory constraints', 'NSEC memory constraints']\n",
    "model_lists = [['SEC_ltm1000', 'SEC_ltm125', 'SEC_ltm250', 'SEC_ltm500'],\n",
    "               ['NSEC_ltm1000', 'NSEC_ltm125', 'NSEC_ltm250', 'NSEC_ltm500']]\n",
    "base_models = ['SEC_ltm1000', 'NSEC_ltm1000']\n",
    "\n",
    "# Define the function to generate and save/display the comparison tables\n",
    "from IPython.display import display\n",
    "\n",
    "def generate_and_display_memory_tables(task_dfs, task_names, model_lists, base_models):\n",
    "    \"\"\"\n",
    "    Generate two comparison tables with different base models for each dataset,\n",
    "    display them, and save them as CSV files.\n",
    "\n",
    "    Parameters:\n",
    "    - task_dfs: List of DataFrames corresponding to the tasks.\n",
    "    - task_names: List of task names.\n",
    "    - model_lists: List of model names for each task.\n",
    "    - base_models: List of base models to use for each dataset.\n",
    "    \"\"\"\n",
    "    comparison_data_sec = []\n",
    "    comparison_data_nsec = []\n",
    "\n",
    "    for task_df, task_name, models, base_model in zip(task_dfs, task_names, model_lists, base_models):\n",
    "        task_data = {'Task': task_name}\n",
    "        \n",
    "        # Check if the base model exists in the dataframe\n",
    "        if base_model in task_df['Model'].values:\n",
    "            base_mean = task_df[task_df['Model'] == base_model]['Mean'].values[0]\n",
    "        else:\n",
    "            print(f\"Base model {base_model} not found in {task_name}\")\n",
    "            continue  # Skip this task if base model not found\n",
    "\n",
    "        best_model = None\n",
    "        best_mean = -float('inf')\n",
    "\n",
    "        # Find the best performing model\n",
    "        for model in models:\n",
    "            if model in task_df['Model'].values:\n",
    "                model_mean = task_df[task_df['Model'] == model]['Mean'].values[0]\n",
    "                if model_mean > best_mean:\n",
    "                    best_mean = model_mean\n",
    "                    best_model = model\n",
    "            else:\n",
    "                print(f\"Model {model} not found in {task_name}\")\n",
    "\n",
    "        # Populate comparison data, with formatting\n",
    "        for model in models:\n",
    "            if model in task_df['Model'].values:\n",
    "                model_mean = task_df[task_df['Model'] == model]['Mean'].values[0]\n",
    "                is_significant = task_df[task_df['Model'] == model]['Significant'].values[0] == 'No'\n",
    "\n",
    "                if model == best_model:\n",
    "                    # Highlight best performing model (bold + underline)\n",
    "                    task_data[model] = f\"**__{model_mean:.2f}__**\"\n",
    "                elif not is_significant:\n",
    "                    # Highlight models not statistically significant from base model (bold)\n",
    "                    task_data[model] = f\"**{model_mean:.2f}**\"\n",
    "                else:\n",
    "                    # Normal formatting for significant models\n",
    "                    task_data[model] = f\"{model_mean:.2f}\"\n",
    "            else:\n",
    "                task_data[model] = 'N/A'  # If model not found, mark as N/A\n",
    "        \n",
    "        # Append to the appropriate comparison list\n",
    "        if base_model == \"SEC_ltm1000\":\n",
    "            comparison_data_sec.append(task_data)\n",
    "        else:\n",
    "            comparison_data_nsec.append(task_data)\n",
    "    \n",
    "    # Create DataFrames from comparison data for each base model\n",
    "    comparison_df_sec = pd.DataFrame(comparison_data_sec)\n",
    "    comparison_df_nsec = pd.DataFrame(comparison_data_nsec)\n",
    "\n",
    "    # Display and save each DataFrame\n",
    "    def display_and_save_table(df, table_name, filename):\n",
    "        print(f\"\\n**{table_name}**\\n\")\n",
    "        display(df.style.set_caption(table_name).hide_index().set_table_styles(\n",
    "            [\n",
    "                {'selector': 'th', 'props': [('font-size', '12pt'), ('text-align', 'center')]},\n",
    "                {'selector': 'td', 'props': [('font-size', '12pt'), ('text-align', 'center')]}\n",
    "            ]\n",
    "        ).set_properties(**{'text-align': 'center'}))\n",
    "        \n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Table saved as {filename}\")\n",
    "\n",
    "    # Display and save SEC memory constraints table\n",
    "    display_and_save_table(comparison_df_sec, \"SEC Memory Constraints Comparison\", \"SEC_memory_constraints_comparison.csv\")\n",
    "\n",
    "    # Display and save NSEC memory constraints table\n",
    "    display_and_save_table(comparison_df_nsec, \"NSEC Memory Constraints Comparison\", \"NSEC_memory_constraints_comparison.csv\")\n",
    "\n",
    "# Run the function with the loaded data\n",
    "generate_and_display_memory_tables(task_dfs, task_names, model_lists, base_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
